{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration i\n",
    "\n",
    "---\n",
    "Use pip to install the necessary packages instead of using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration ii\n",
    "\n",
    "---\n",
    "Following Section only need to be run at the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "import json, regex\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "async def __constant_update():\n",
    "    async with ClientSession(raise_for_status=True,headers=headers) as c:\n",
    "        async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "            _script = await res.text()\n",
    "\n",
    "        with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"_FORMS = \")\n",
    "            json.dump({\n",
    "                form.pop(\"form\"): form\n",
    "                for form in eval(regex.search(\n",
    "                    R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                    _script,\n",
    "                    regex.MULTILINE\n",
    "                )[1])\n",
    "            }, f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "            f.write(\"_LOCATIONS = \")\n",
    "            json.dump(dict(eval(regex.search(\n",
    "                R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "                _script,\n",
    "                regex.MULTILINE\n",
    "            )[1])), f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "await __constant_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry\n",
    "import time\n",
    "from tqdm import *\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "@retry\n",
    "async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "    q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "    #forms = \" \".join(form for  form in forms)\n",
    "    data = {'q':q,\n",
    "            'startdt':start,\n",
    "            'enddt':end,\n",
    "            'ciks':cik,\n",
    "            'dataRange':'custom',\n",
    "            'category':'custom',\n",
    "            'forms':forms}\n",
    "    url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    async with semaphore,client.request(method='get',url=url,params = data) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 200:\n",
    "            result = await res.read()\n",
    "            fetch_bar.update(1)\n",
    "            return result#await res.json()\n",
    "        raise ValueError(f\"Status Code = {res.status}\")\n",
    "\n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "@retry\n",
    "async def _download(semaphore: asyncio.Semaphore, row_index, df,client,keywords,download_bar):\n",
    "    url = df.loc[row_index,\"url\"]\n",
    "    ext = df.loc[row_index,\"file_ext\"]\n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            #print(\"ok\")\n",
    "            html = await res.read()\n",
    "        if ext == \"htm\":\n",
    "            paragraphs = extract_html(html,keywords)\n",
    "        elif ext == \"pdf\":\n",
    "            paragraphs = extract_pdf(html,keywords)\n",
    "        elif ext == \"txt\":\n",
    "            paragraphs = extract_txt(html,keywords)\n",
    "        else:\n",
    "            tem = f\"paragrah{1}\"\n",
    "            df.loc[row_index,tem] = 'Unknow extension'\n",
    "            res.raise_for_status()\n",
    "            return download_bar.update(1)\n",
    "        if paragraphs:\n",
    "            for num in range(len(paragraphs)):\n",
    "                tem = f\"paragrah{num + 1}\"\n",
    "                df.loc[row_index,tem] = str(paragraphs[num])\n",
    "            return download_bar.update(1)\n",
    "        _LOGGER.warning(f\"{url} file will be skipped: ({res.status}) {res.reason}\")\n",
    "        tem = f\"paragrah{1}\"\n",
    "        df.loc[row_index,tem] = 'Download skipped'\n",
    "        return download_bar.update(1)\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]): \n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    \n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), str(loc).split(\",\")[0], _LOCATIONS.get(code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"biz_states\"], #source[\"inc_states\"] if source[\"inc_states\"] else \n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        ) \n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], copy=False)#, dtype=str\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows.loc[0,\"cik\"]\n",
    "    info = pd.DataFrame({\n",
    "        \"entity_name\":rows['entity_name'],\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks, adsh.replace('-', ''), filename),\n",
    "        \"parser\": None#getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    },copy=False,dtype=str)#, dtype=object\n",
    "    \n",
    "    result = pd.merge(rows,info,how=\"left\",on=\"id\")\n",
    "    #result.drop_duplicates(inplace=True)\n",
    "    #del result[\"id\"]\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def extract_html(html, keywords):\n",
    "    soup = BeautifulSoup(html, \"lxml\")#'html.parser'\n",
    "    #extracted_text = soup.get_text(strip=True)\n",
    "    matching_paragraphs = [\n",
    "    paragraph.get_text()\n",
    "    for paragraph in soup.find_all(True)#[\"tr\",\"td\",\"span\",\"p\",\"table\"]\n",
    "    if any(keyword in paragraph.get_text() for keyword in keywords ) and len(paragraph.get_text())<=10000\n",
    "]\n",
    "    return list(set(matching_paragraphs))\n",
    "\n",
    "def extract_pdf(fetch,keywords):\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    #pattern = regex.compile(r'(\\n\\W*\\n)', regex.V1)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=\"pdf\")\n",
    "    line_separator_pattern = regex.compile(r'(\\n\\W*?\\n)')\n",
    "    potential_line_separators = list()\n",
    "\n",
    "    for page in range(pdf_document.page_count):\n",
    "        text = pdf_document[page].get_text(\"text\")\n",
    "        matches = line_separator_pattern.findall(text)\n",
    "        potential_line_separators.extend(matches)\n",
    "\n",
    "    # Use the most common line separator as the final choice\n",
    "    if potential_line_separators:\n",
    "        line_separator = max(potential_line_separators, key=potential_line_separators.count)\n",
    "    else:\n",
    "        # Default to the original separator if none is found\n",
    "        line_separator = \"\\n  \\n\"\n",
    "    total = [para for page in range(pdf_document.page_count) \n",
    "             for para in regex.split(line_separator, pdf_document[page].get_text(\"text\")) \n",
    "             if any(keyword in para for keyword in keywords)]\n",
    "    return total\n",
    "def extract_txt(bytes,keywords):\n",
    "    txt = bytes.decode('utf-8').split('\\n\\n')\n",
    "    txt = [i.replace(\"\\n\",\" \")for i in txt]\n",
    "    matching_paragraphs = [paragraph for paragraph in txt if any(keyword in paragraph.lower() for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "def CIK(file):\n",
    "    with open(file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "            return _ciks\n",
    "        except IOError as e:\n",
    "                raise ValueError(f\"{file} is not a valid file\") from e\n",
    "    # if _ciks:\n",
    "    #     for i in range(0, len(_ciks), ciks_per_query):\n",
    "    #         yield _ciks[i:i + ciks_per_query]\n",
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    # if total_hits == 0:\n",
    "    #     return None\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits\n",
    "\n",
    "\n",
    "\n",
    "async def main(_PHRASES,_FILING_TYPES,_DATE_START,_DATE_END,_CIKS_PER_QUERY, _CIKS,_buffer_chunk_size,df):\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "    if _FILING_TYPES == []:\n",
    "        _FILING_TYPES = [\"\"]\n",
    "    #-------------- Crawl --------\n",
    "    async with ClientSession(raise_for_status=True, headers=headers) as client :\n",
    "        #------Fetch--------\n",
    "        total = len(_CIKS) * len(_FILING_TYPES)\n",
    "        with tqdm(        \n",
    "            total=total) as fetch_bar:\n",
    "            print(\"Starting fetch...\")\n",
    "            fetch_tasks = [\n",
    "                    asyncio.create_task(fetch(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        phrases=_PHRASES,\n",
    "                        cik=cik,\n",
    "                        start=_DATE_START,\n",
    "                        end=_DATE_END,\n",
    "                        forms=form,\n",
    "                        fetch_bar=fetch_bar\n",
    "                    ))\n",
    "                    for  form in _FILING_TYPES for cik in _CIKS\n",
    "                ]\n",
    "            fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "            df = pd.concat([df] + \n",
    "                        [_parse_hit(hit) for data in fetched_data for hit in decode(data)],\n",
    "                        ignore_index=True)\n",
    "            df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "        #-----Docs download-------\n",
    "        print(f\"fetch completed and collected [{df.shape[0]}] of docs,starting download docs..\")\n",
    "        total = df.shape[0]\n",
    "        if total == 0:\n",
    "            print(\"Fetch completed with 0 result. Now existing.\")\n",
    "            return\n",
    "        with tqdm(total=total) as download_bar:\n",
    "            for index in range(0,total,_buffer_chunk_size):\n",
    "                index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "                download_tasks = [\n",
    "                    asyncio.create_task(_download(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        df = df,\n",
    "                        row_index=row,\n",
    "                        download_bar=download_bar,\n",
    "                        keywords=_PHRASES)\n",
    "                    )for row in index_range\n",
    "                    ]\n",
    "                downloaded = await asyncio.gather(*download_tasks)\n",
    "                \n",
    "\n",
    "\n",
    "    del df[\"parser\"] \n",
    "    del df['id']\n",
    "    df.to_excel(f\"{str(date.today())}.xlsx\")   \n",
    "    print(f\"Data have been export at {os.getcwd()}\\{str(date.today())}.xlsx\") \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:14<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch completed and collected [311] of docs,starting download docs..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [03:58<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data have been export at /mnt/d/summer_research\\2023-12-22.xlsx\n",
      "--------------------\n",
      "All tasks completed! Time Cost:4.2 minutes \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "T0 = time.time()\n",
    "\n",
    "_PHRASES = [\"data \",\"cyber\"]\n",
    "\n",
    "_FILING_TYPES = [\"\"]#,\n",
    "#_FILING_TYPES = [\"\"10-K\",\"10-Q\"\"]\n",
    "_DATE_START = \"2001-01-01\"\n",
    "_DATE_END = \"2023-12-12\"\n",
    "_CIKS_PER_QUERY = 10\n",
    "_buffer_chunk_size = 200\n",
    "#_CIKS =  [\"0001653482\"] #Input from a list or a path,,\"0001653481\"\n",
    "_CIKS = CIK(Path(\"sample_input_file.txt\"))[2400:] ####Please mannually select N here for Testing for the first N CIKs###\n",
    "df = pd.DataFrame()\n",
    "df = await main(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _CIKS,\n",
    "    _buffer_chunk_size,\n",
    "    df\n",
    ")\n",
    " \n",
    "END = time.time()\n",
    "print(\"--\"*10,\n",
    "      f\"All tasks completed! Time Cost:{round((END-T0)/60,1)} minutes \",sep='\\n')#pymupdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
