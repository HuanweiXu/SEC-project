{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration i \n",
    "\n",
    "----\n",
    "Update python to 3.10.13 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install --yes -c defaults -c conda-forge --update-all python=3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration ii\n",
    "\n",
    "---\n",
    "\n",
    "Uninstall package fitz, which may cause import conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iii\n",
    "\n",
    "---\n",
    "\n",
    "Go to your conda's base directory(e.g. `D:\\anaconda3`),under the path `D:\\anaconda3\\Lib\\site-packages`, **delete** all folders which name contain \"fitz\" if there any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iv\n",
    "\n",
    "---\n",
    "Use pip to install the necessary packages instead of using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp>=3.9.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: Faker>=18.9.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (22.0.0)\n",
      "Requirement already satisfied: pandas>=2.1.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.1.4)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2023.12.25)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (8.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: PyMuPDF>=1.23.8 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.23.8)\n",
      "Requirement already satisfied: ipykernel>=2.0.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (6.25.0)\n",
      "Collecting asyncio>=3.4.3 (from -r requirements.txt (line 10))\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from beautifulsoup4>=4.12.2->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from Faker>=18.9.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (1.26.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (2023.4)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from PyMuPDF>=1.23.8->-r requirements.txt (line 8)) (1.23.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (8.15.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.5.6)\n",
      "Requirement already satisfied: packaging in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (23.1)\n",
      "Requirement already satisfied: psutil in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (4.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=2.0.0->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from python-dateutil>=2.4->Faker>=18.9.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.9.0->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.2)\n",
      "Installing collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration v\n",
    "\n",
    "---\n",
    "press **Ctrl** + **SHFIT** + **P**, type **reload window** and select to reboot the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration vi\n",
    "\n",
    "---\n",
    "Following Section only need to be run at the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "import json, regex\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "async def __constant_update():\n",
    "    async with ClientSession(raise_for_status=True,headers=headers) as c:\n",
    "        async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "            _script = await res.text()\n",
    "\n",
    "        with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"_FORMS = \")\n",
    "            json.dump({\n",
    "                form.pop(\"form\"): form\n",
    "                for form in eval(regex.search(\n",
    "                    R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                    _script,\n",
    "                    regex.MULTILINE\n",
    "                )[1])\n",
    "            }, f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "            f.write(\"_LOCATIONS = \")\n",
    "            json.dump(dict(eval(regex.search(\n",
    "                R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "                _script,\n",
    "                regex.MULTILINE\n",
    "            )[1])), f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "await __constant_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession,ClientResponseError\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry,stop_after_attempt, wait_fixed,after_log,RetryError\n",
    "#import retry\n",
    "import time\n",
    "from tqdm import *\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "_DEFAULT_BYTE = b'{\"took\":530,\"timed_out\":false,\"_shards\":{\"total\":50,\"successful\":50,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":0,\"relation\":\"eq\"},\"max_score\":null,\"hits\":[]},\"aggregations\":{\"entity_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"sic_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"biz_states_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"form_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]}},\"query\":\"{\\\\\"query\\\\\":{\\\\\"bool\\\\\":{\\\\\"must\\\\\":[{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"data\\\\\"}},{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"breach\\\\\"}}],\\\\\"must_not\\\\\":[],\\\\\"should\\\\\":[],\\\\\"filter\\\\\":[{\\\\\"terms\\\\\":{\\\\\"ciks\\\\\":[\\\\\"0001199046\\\\\"]}},{\\\\\"range\\\\\":{\\\\\"file_date\\\\\":{\\\\\"gte\\\\\":\\\\\"2024-01-02\\\\\",\\\\\"lte\\\\\":\\\\\"2024-01-02\\\\\"}}}]}},\\\\\"from\\\\\":0,\\\\\"size\\\\\":100,\\\\\"aggregations\\\\\":{\\\\\"form_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"root_form\\\\\",\\\\\"size\\\\\":30}},\\\\\"entity_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"display_names.raw\\\\\",\\\\\"size\\\\\":30}},\\\\\"sic_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"sics\\\\\",\\\\\"size\\\\\":30}},\\\\\"biz_states_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"biz_states\\\\\",\\\\\"size\\\\\":30}}}}\"}'\n",
    "\n",
    "# @retry(stop=stop_after_attempt(10), wait=wait_fixed(2),reraise=False)\n",
    "# async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "#     q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "#     #forms = \" \".join(form for  form in forms)\n",
    "#     data = {'q':q,\n",
    "#             'startdt':start,\n",
    "#             'enddt':end,\n",
    "#             'ciks':cik,\n",
    "#             'dataRange':'custom',\n",
    "#             'category':'custom',\n",
    "#             'forms':forms}\n",
    "#     url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    \n",
    "#     async with semaphore, client.request(method='get', url=url, params=data) as res:\n",
    "#         await asyncio.sleep(1)\n",
    "#         try:\n",
    "#             if res.status == 200:\n",
    "#                 result = await res.read()\n",
    "#                 fetch_bar.update(1)\n",
    "#                 return result\n",
    "#         except  RetryError as e: #ClientResponseError or\n",
    "#             _LOGGER.warning(f\"{cik}-{start}-{end} query will be skipped: ({res.status}) {res.reason}\")\n",
    "#             fetch_bar.update(1)\n",
    "#             return _DEFAULT_BYTE\n",
    "    \n",
    "@retry(wait=wait_fixed(1))\n",
    "async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "    q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "    #forms = \" \".join(form for  form in forms)\n",
    "    data = {'q':q,\n",
    "            'startdt':start,\n",
    "            'enddt':end,\n",
    "            'ciks':cik,\n",
    "            'dataRange':'custom',\n",
    "            'category':'custom',\n",
    "            'forms':forms}\n",
    "    url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    async with semaphore,client.request(method='get',url=url,params = data) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 200:\n",
    "            result = await res.read()\n",
    "            fetch_bar.update(1)\n",
    "            return result#await res.json()\n",
    "        raise ValueError(f\"Status Code = {res.status}\")\n",
    "    \n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(6), wait=wait_fixed(2),after=after_log(_LOGGER, logging.WARNING),reraise=False)#, \n",
    "async def _download(semaphore: asyncio.Semaphore, row_index, df,client,keywords,download_bar):\n",
    "    url = df.loc[row_index,\"url\"]\n",
    "    ext = df.loc[row_index,\"file_ext\"]\n",
    "    if ext not in [\"htm\",\"pdf\",\"txt\"]:\n",
    "        _LOGGER.warning(f\"Unkown extension in row [{row_index}]\")\n",
    "        tem = f\"paragrah{1}\"\n",
    "        df.loc[row_index,tem] = 'Unknow extension'\n",
    "        return download_bar.update(1)\n",
    "        \n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 404:\n",
    "            _LOGGER.warning(f\"Url: {url} is not exist: ({res.status}) {res.reason}. Skipping download.\")\n",
    "            return download_bar.update(1)\n",
    "        if res.status == 403 or res.status == 429:\n",
    "            _LOGGER.warning(f\"Reach request limitation: {url}: ({res.status}) {res.reason}.\\n Restart to crawl after 10 minutes. \")\n",
    "            await asyncio.sleep(630)\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                confirm = input(\"Please open 'https://www.sec.gov/edgar/search/' in your browser, press 'ok' to crawl continue if it available\")\n",
    "                if confirm.lower() == \"ok\":\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Manully confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        _LOGGER.warning(f\"Fail to Manully confirm: ({res.status}) {res.reason} \")\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - start_time\n",
    "                if elapsed_time >= 180 :\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Automatically confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        _LOGGER.warning(\"Fail to automate confirm. Will retry again after 10 mintues\")\n",
    "                        await asyncio.sleep(600)\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "        if res.ok:\n",
    "            try:\n",
    "                html = await res.read()\n",
    "                if ext == \"htm\":\n",
    "                    paragraphs = extract_html(html,keywords)\n",
    "                elif ext == \"pdf\":\n",
    "                    paragraphs = extract_pdf(html,keywords)\n",
    "                elif ext == \"txt\":\n",
    "                    paragraphs = extract_txt(html,keywords)\n",
    "                if not paragraphs:\n",
    "                    _LOGGER.warning(f\"No content extracted from {url}\")\n",
    "                    tem = f\"paragrah{1}\"\n",
    "                    df.loc[row_index, tem] = 'No content extracted'\n",
    "                    return download_bar.update(1)\n",
    "    \n",
    "                for num in range(len(paragraphs)):\n",
    "                    tem = f\"paragrah{num + 1}\"\n",
    "                    df.loc[row_index,tem] = str(paragraphs[num])\n",
    "                return download_bar.update(1)\n",
    "            except RetryError:\n",
    "                _LOGGER.warning(f\"Retring download url: {url} at the row: {row_index}\")\n",
    "                return download_bar.update(1)\n",
    "\n",
    "    _LOGGER.warning(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    tem = f\"paragrah{1}\"\n",
    "    df.loc[row_index, tem] = 'Download skipped'\n",
    "    #raise Exception(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    return download_bar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]): \n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    \n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), str(loc).split(\",\")[0], _LOCATIONS.get(code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"biz_states\"], #source[\"inc_states\"] if source[\"inc_states\"] else \n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        ) \n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], copy=False)#, dtype=str\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows.loc[0,\"cik\"]\n",
    "    info = pd.DataFrame({\n",
    "        \"entity_name\":rows['entity_name'],\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks, adsh.replace('-', ''), filename),\n",
    "        \"parser\": None#getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    },copy=False,dtype=str)#, dtype=object\n",
    "    \n",
    "    result = pd.merge(rows,info,how=\"left\",on=\"id\")\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "#len_check = lambda chunk,sub_delimiter: [chunk] if len(chunk) <=3000 else chunk.split(sub_delimiter)\n",
    "\n",
    "def extract_html(html, keywords):\n",
    "    matching_paragraphs = [regex.sub(r\"(\\s+)\",\" \",para).strip() \n",
    "                           for para in BeautifulSoup(html, \"lxml\",from_encoding='utf-8').get_text('\\n\\n').split('\\n\\n') \n",
    "                           if any(keyword in para.lower().replace(\"-\",\" \").replace(\"\\n\",\" \") \n",
    "                            for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "def extract_pdf(fetch,keywords):\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=\"pdf\")\n",
    "\n",
    "    total = [para.replace(\"\\n\",\" \") for page in range(pdf_document.page_count) \n",
    "            for lists in pdf_document[page].get_text(\"blocks\") \n",
    "            for  para in lists \n",
    "            if any(keyword in str(para).lower().replace(\"-\",\" \") for keyword in keywords) ]\n",
    "    return total\n",
    "\n",
    "def extract_txt(bytes,keywords):\n",
    "    txt = bytes.decode('utf-8').split('\\n\\n')\n",
    "    txt = [i.replace(\"\\n\",\" \")for i in txt]\n",
    "    matching_paragraphs = [paragraph for paragraph in txt if any(keyword in paragraph.lower().replace(\"-\",\" \") for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "    \n",
    "def chop_time_interval(start_date,end_date,frequency = \"2D\"):\n",
    "    interval = pd.interval_range(pd.Timestamp(start_date) ,pd.Timestamp(end_date),closed=\"both\",freq=frequency)\n",
    "    date_pattern = '%Y-%m-%d'\n",
    "    intervals = [(interval[i].left.strftime((date_pattern)), interval[i].right.strftime((date_pattern))) for i in range(len(interval)) if i % 2 ==0 ]\n",
    "    return intervals\n",
    "\n",
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    if total_hits >= 100:\n",
    "        _LOGGER.warning(f\"Numbers opf Hits/entries >= 100. Try to reduce the interval of data range to avoid data missing.\")\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits\n",
    "\n",
    "def CIK(ciks):\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "    if _ciks == []:\n",
    "        _ciks = \"\"\n",
    "    return _ciks\n",
    "    \n",
    "async def main(_PHRASES,\n",
    "               _FILING_TYPES,\n",
    "               _DATE_START,\n",
    "               _DATE_END,\n",
    "               _CIKS_PER_QUERY, \n",
    "               _CIKS,\n",
    "               _buffer_chunk_size,\n",
    "               df,headers,\n",
    "               _OUTPUT_NAME,\n",
    "               _OUTPUT_FORMAT,\n",
    "               _DATA_RANGE_INTERVAL=\"2D\"):\n",
    "    if _OUTPUT_FORMAT not in [\"excel\",\"xlsx\",\"csv\"]:\n",
    "        raise TypeError(f\"{_OUTPUT_FORMAT} is not a valid extension. Options extension [excel,xlsx,csv].\")\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    if _FILING_TYPES == []:\n",
    "        _FILING_TYPES = [\"\"]\n",
    "    #-------------- Crawl --------\n",
    "\n",
    "    async with ClientSession(raise_for_status=False, headers=headers) as client :\n",
    "        #------Fetch--------\n",
    "        if _CIKS:\n",
    "            total = len(_CIKS) * len(_FILING_TYPES)\n",
    "            with tqdm(        \n",
    "                total=total) as fetch_bar:\n",
    "                print(\"Starting fetch...\")\n",
    "                fetch_tasks = [\n",
    "                        asyncio.create_task(fetch(\n",
    "                            semaphore=semaphore,\n",
    "                            client=client,\n",
    "                            phrases=_PHRASES,\n",
    "                            cik=cik,\n",
    "                            start=_DATE_START,\n",
    "                            end=_DATE_END,\n",
    "                            forms=form,\n",
    "                            fetch_bar=fetch_bar\n",
    "                        ))\n",
    "                        for  form in _FILING_TYPES for cik in _CIKS\n",
    "                    ]\n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        else:\n",
    "            intervals = chop_time_interval(_DATE_START,_DATE_END,_DATA_RANGE_INTERVAL)\n",
    "            with tqdm(total=len(intervals)) as fetch_bar:\n",
    "                fetch_tasks = [asyncio.create_task(fetch(\n",
    "                                    semaphore=semaphore,\n",
    "                                    client=client,\n",
    "                                    phrases=_PHRASES,\n",
    "                                    cik=\"\",\n",
    "                                    start=start,\n",
    "                                    end=end,\n",
    "                                    forms=_FILING_TYPES,\n",
    "                                    fetch_bar=fetch_bar\n",
    "                                )) for start,end in intervals]\n",
    "                \n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        df = pd.concat([df] + \n",
    "                    [_parse_hit(hit) for data in fetched_data \n",
    "                     for hit in decode(data)],\n",
    "                    ignore_index=True)\n",
    "        df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "        df = df.sample(frac=1.0)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        #-----Docs download-------\n",
    "        print(f\"fetch completed and collected [{df.shape[0]}] of docs,starting download docs..\")\n",
    "        total = df.shape[0]\n",
    "        if total == 0:\n",
    "            print(\"Fetch completed with 0 result. Now existing.\")\n",
    "            return\n",
    "        with tqdm(total=total) as download_bar:\n",
    "            for index in range(0,total,_buffer_chunk_size):\n",
    "                index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "                download_tasks = [\n",
    "                    asyncio.create_task(_download(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        df = df,\n",
    "                        row_index=row,\n",
    "                        download_bar=download_bar,\n",
    "                        keywords=_PHRASES)\n",
    "                    )for row in index_range\n",
    "                    ]\n",
    "                downloaded = await asyncio.gather(*download_tasks)\n",
    "                \n",
    "\n",
    "\n",
    "    del df[\"parser\"] \n",
    "    del df['id']\n",
    "    if _OUTPUT_FORMAT in [\"excel\",\"xlsx\"]:\n",
    "        df.to_excel(f\"{_OUTPUT_NAME}.xlsx\")\n",
    "    elif _OUTPUT_FORMAT.lower() == \"csv\":      \n",
    "        df.to_csv(f\"{_OUTPUT_NAME}.csv\")\n",
    "    print(f\"Data have been export at {os.getcwd()}\") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "- **df** (`pandas dataframe`) : Output data frame.\n",
    "\n",
    "- **headers** (`str`) : A string of user's email address for declaration.\n",
    "\n",
    "- **PHRASES** (`List[str]`): A list of keywords or phrases to search for.\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **DATA_RANGE_INTERVAL** (`str`): A string to set up a data range interval for per query. Enable when do not providing an specific CIKs input, i.e., \"1D\",\"3D\" and \"**X**D\". Reducing the interval will result in more queires have to be made. Recommanded set as not more than \"2D\" since may return more than 100 enties per query and casue data missing.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE**: The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT** (`enumerate[\"excel\",\"xlsx\",\"csv\"]`): The file format of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2099 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2099/2099 [14:52<00:00,  2.35it/s]\n",
      "mismatched CIK: 0001199046 (parsed from \"Pear Tree Funds  (CIK 0001199046)\") v.s. 0000722885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch completed and collected [7571] of docs,starting download docs..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 7341/7571 [1:52:46<06:10,  1.61s/it]  Url: https://www.sec.gov/Archives/edgar/data/0001539031/000091205714000136/filename1.htm is not exist: (404) Not Found. Skipping download.\n",
      "100%|██████████| 7571/7571 [1:56:34<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data have been export at /mnt/d/summer_research\n",
      "--------------------\n",
      "All tasks completed! Time Cost:132.0 minutes \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "T0 = time.time()\n",
    "df = pd.DataFrame()\n",
    "headers = {\"User-Agent\":\"a1835067@student.adelaide.edu.au\"}\n",
    "_PHRASES = [\"data breach\", \"cyber security\"]\n",
    "_DATA_RANGE_INTERVAL = \"2D\" # \n",
    "#_FILING_TYPES = [\"\"]#,\n",
    "_FILING_TYPES = [\"\"]\n",
    "_DATE_START = \"2001-01-01\"\n",
    "_DATE_END = \"2023-12-27\"\n",
    "_CIKS_PER_QUERY = 10\n",
    "_BUFFER_CHUNK_SIZE = 10\n",
    "_CIKS =  [] #Input from a list or a path,,\"0001653481\"\n",
    "#_CIKS = CIK(Path(\"sample_input_file.txt\"))[2400:] ####Please mannually select N here for Testing for the first N CIKs###\n",
    "_OUTPUT_NAME = \"20240102_noCKIs_full\"\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "\n",
    "df = await main(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _CIKS,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    df,\n",
    "    headers,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT,\n",
    "    _DATA_RANGE_INTERVAL\n",
    ")\n",
    " \n",
    "END = time.time()\n",
    "print(\"--\"*10,\n",
    "      f\"All tasks completed! Time Cost:{round((END-T0)/60,1)} minutes \",sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#print(hits)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hits\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(byte)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(byte):\n\u001b[0;32m----> 2\u001b[0m     total_hits \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# if total_hits == 0:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#     return None\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     hits \u001b[38;5;241m=\u001b[39m  json\u001b[38;5;241m.\u001b[39mloads(byte\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    # if total_hits == 0:\n",
    "    #     return None\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits\n",
    "decode(b'')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
