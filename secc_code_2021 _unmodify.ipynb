{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "musical-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "     active environment : UoA-ABS-RA\n",
      "    active env location : C:\\Users\\laitingsheng\\miniconda3\\envs\\UoA-ABS-RA\n",
      "            shell level : 2\n",
      "       user config file : C:\\Users\\laitingsheng\\.condarc\n",
      " populated config files : C:\\Users\\laitingsheng\\.condarc\n",
      "          conda version : 4.10.3\n",
      "    conda-build version : not installed\n",
      "         python version : 3.8.10.final.0\n",
      "       virtual packages : __cuda=11.4=0\n",
      "                          __win=0=0\n",
      "                          __archspec=1=x86_64\n",
      "       base environment : C:\\Users\\laitingsheng\\miniconda3  (writable)\n",
      "      conda av data dir : C:\\Users\\laitingsheng\\miniconda3\\etc\\conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "                          https://conda.anaconda.org/conda-forge/win-64\n",
      "                          https://conda.anaconda.org/conda-forge/noarch\n",
      "          package cache : C:\\Users\\laitingsheng\\miniconda3\\pkgs\n",
      "                          C:\\Users\\laitingsheng\\.conda\\pkgs\n",
      "                          C:\\Users\\laitingsheng\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\Users\\laitingsheng\\miniconda3\\envs\n",
      "                          C:\\Users\\laitingsheng\\.conda\\envs\n",
      "                          C:\\Users\\laitingsheng\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/4.10.3 requests/2.25.1 CPython/3.8.10 Windows/10 Windows/10.0.22000\n",
      "          administrator : False\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install --yes -c defaults -c conda-forge --update-all python=3.8 aiohttp aiodns bs4 cchardet Faker lxml openpyxl pandas PyPDF2 python-dateutil regex tenacity\n",
    "%conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77361c4-f9e6-47d2-adb9-d6881fac2ed1",
   "metadata": {},
   "source": [
    "# SEC Constants\n",
    "\n",
    "This section has been commented out since it doesn't need to be run frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14f2031-3d0f-4842-9d41-4504fd838ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "#\n",
    "# from aiohttp import ClientSession\n",
    "#\n",
    "# async with ClientSession(raise_for_status=True) as c:\n",
    "#     async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "#         _script = await res.text()\n",
    "#\n",
    "#     with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(\"_FORMS = \")\n",
    "#         json.dump({\n",
    "#             form.pop(\"form\"): form\n",
    "#             for form in eval(re.search(\n",
    "#                 R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "#                 _script,\n",
    "#                 re.MULTILINE\n",
    "#             )[1])\n",
    "#         }, f, indent=4)\n",
    "#\n",
    "#         f.write('\\n')\n",
    "#\n",
    "#         f.write(\"_LOCATIONS = \")\n",
    "#         json.dump(dict(eval(re.search(\n",
    "#             R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "#             _script,\n",
    "#             re.MULTILINE\n",
    "#         )[1])), f, indent=4)\n",
    "#\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81373d78-1f07-41f5-87be-153c75604e6d",
   "metadata": {},
   "source": [
    "## Common Functions\n",
    "\n",
    "The `chop_periods` function may be replaced by pd.interval_range once the functionality is enhanced in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ad37e6-80df-40f6-844b-570cc1c7da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from typing import Any, Callable, Dict, Generator, Optional, Tuple\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "def chop_periods(\n",
    "    start_date: date,\n",
    "    end_date: date,\n",
    "    interval: Optional[Dict[str, Any]],\n",
    "    _format: Callable[[date], str]\n",
    ") -> Generator[Tuple[str, str], None, None]:\n",
    "    if interval is None:\n",
    "        yield _format(start_date), _format(end_date)\n",
    "        return\n",
    "\n",
    "    delta = relativedelta(**interval)\n",
    "    # the end date of each period must have an offset of -1 day since the RESTful API is inclusive in both sides\n",
    "    offset = relativedelta(days=1)\n",
    "    next_date = start_date\n",
    "    while (next_date := (curr_date := next_date) + delta) < end_date:\n",
    "        yield _format(curr_date), _format(next_date - offset)\n",
    "    yield _format(curr_date), _format(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a0701-f085-435c-8cda-7b2000ec549f",
   "metadata": {},
   "source": [
    "## Primary Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d279156-4771-48c7-960c-89b6a83cec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry\n",
    "\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import parsers\n",
    "\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "\n",
    "# parse the name and CIK from the result (same as the official JavaScript)\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "\n",
    "_FAKER = Faker()\n",
    "\n",
    "_FORMAT_SPEC = {\n",
    "    \"csv\": {\n",
    "        \"suffix\": \"csv\",\n",
    "        \"function_suffix\": \"csv\",\n",
    "        \"extra_args\": {\n",
    "            \"encoding\": \"utf-8\"\n",
    "        }\n",
    "    },\n",
    "    \"excel\": {\n",
    "        \"suffix\": \"xlsx\",\n",
    "        \"function_suffix\": \"excel\",\n",
    "        \"extra_args\": {\n",
    "            \"sheet_name\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "\n",
    "# Replace by a single space in text\n",
    "_WHITESPACE_REGEX = regex.compile(R\"\\s+\", regex.V1)\n",
    "\n",
    "\n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "\n",
    "def _decode(b: bytes, e: str) -> str:\n",
    "    return b.decode(e)\n",
    "\n",
    "\n",
    "@retry\n",
    "async def _download(client: ClientSession, semaphore: asyncio.Semaphore, url: str) -> Tuple[bytes, str]:\n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            return await res.read(), res.get_encoding()\n",
    "        if res.status in _RETRY_SC:\n",
    "            res.raise_for_status()\n",
    "    _LOGGER.warning(f\"{url} file will be skipped: ({res.status}) {res.reason}\")\n",
    "    return b'', \"ascii\"\n",
    "\n",
    "\n",
    "@retry\n",
    "async def _fetch(\n",
    "    client: ClientSession,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    fixed_query: Dict[str, Any],\n",
    "    ciks: Optional[List[str]],\n",
    "    start_date: str,\n",
    "    end_date: str\n",
    ") -> Tuple[bytes, str]:\n",
    "    async with semaphore, client.post(\"https://efts.sec.gov/LATEST/search-index\", json={\n",
    "        **fixed_query,\n",
    "        \"startdt\": start_date,\n",
    "        \"enddt\": end_date,\n",
    "        \"ciks\": ciks\n",
    "    }) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            return await res.read(), res.get_encoding()\n",
    "        if res.status in _RETRY_SC:\n",
    "            res.raise_for_status()\n",
    "        _LOGGER.warning(f\"{ciks}-{start_date}-{end_date} query will be skipped: ({res.status}) {res.reason}\")\n",
    "        return b'', \"ascii\"\n",
    "\n",
    "\n",
    "def _iso(d: date):\n",
    "    return d.isoformat()\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]):\n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), loc, _LOCATIONS.get(code, code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"inc_states\"],\n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        )\n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], dtype=str, copy=False)\n",
    "\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows[\"cik\"]\n",
    "\n",
    "    info = pd.Series({\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks[ciks.notnull()].iloc[-1], adsh.replace('-', ''), filename),\n",
    "        \"parser\": getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    }, dtype=object, copy=False)\n",
    "    return rows, info\n",
    "\n",
    "\n",
    "def _rename(index):\n",
    "    return f\"paragraph{index + 1}\"\n",
    "\n",
    "\n",
    "def _unwrap(hits: Dict[str, Any]):\n",
    "    total_hits = hits[\"total\"]\n",
    "    if total_hits[\"relation\"] == \"gte\":\n",
    "        _LOGGER.warning(f\"The query returns a result exceeding the 10k limit\")\n",
    "    return hits[\"hits\"]\n",
    "\n",
    "\n",
    "def chop_ciks(\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int\n",
    ") -> Generator[Optional[List[str]], None, None]:\n",
    "    # defaults to None\n",
    "    _ciks: Optional[List[str]] = None\n",
    "    # if the provided parameter is a Path, read the CIKs from the file\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "\n",
    "    if _ciks:\n",
    "        for i in range(0, len(_ciks), ciks_per_query):\n",
    "            yield _ciks[i:i + ciks_per_query]\n",
    "    else:\n",
    "        yield None\n",
    "\n",
    "\n",
    "async def crawl(\n",
    "    phrases: List[str],\n",
    "    filing_types: List[str],\n",
    "    start_date: date,\n",
    "    end_date: date,\n",
    "    interval: Optional[Dict[str, int]],\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int,\n",
    "    buffer_chunk_size: int,\n",
    "    output_name: str,\n",
    "    output_format: str\n",
    "):\n",
    "    fixed_query: Dict[str, Any] = {\n",
    "        \"q\": \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases),\n",
    "        \"category\": \"custom\",\n",
    "        \"forms\": filing_types,\n",
    "        \"dateRange\": \"custom\"\n",
    "    }\n",
    "\n",
    "    phrases_regex = regex.compile(\n",
    "        \"|\".join(f\"(?:{phrase})\" for phrase in map(regex.escape, phrases)),\n",
    "        regex.V1 | regex.IGNORECASE\n",
    "    )\n",
    "\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    async with ClientSession() as c:\n",
    "        dfs, infos = zip(*[\n",
    "            _parse_hit(hit)\n",
    "            for task in [\n",
    "                asyncio.create_task(_fetch(c, semaphore, fixed_query, ciks, *period))\n",
    "                for ciks in chop_ciks(ciks, ciks_per_query)\n",
    "                for period in chop_periods(start_date, end_date, interval, _iso)\n",
    "            ]\n",
    "            for hit in _unwrap(json.loads(_decode(*await task))[\"hits\"])\n",
    "        ])\n",
    "\n",
    "        df = pd.concat(dfs, ignore_index=True, copy=False)\n",
    "        df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "        df.set_index(keys=\"id\", inplace=True, verify_integrity=False)\n",
    "        del dfs\n",
    "\n",
    "        info = pd.DataFrame(infos, dtype=object, copy=False)\n",
    "        info.dropna(subset=[\"parser\"], inplace=True)\n",
    "        info.drop_duplicates(subset=\"id\", inplace=True, ignore_index=True)\n",
    "        info.set_index(keys=\"id\", inplace=True)\n",
    "        del infos\n",
    "\n",
    "        dl_info = info[[\"url\", \"parser\"]]\n",
    "        del info[\"parser\"]\n",
    "        downloaded = pd.DataFrame([\n",
    "            pd.Series(filter(phrases_regex.search, (\n",
    "                _CC_REGEX.sub(\"\", _WHITESPACE_REGEX.sub(\" \", s).strip())\n",
    "                for s in parser(*await task).split(\"\\n\\n\")\n",
    "            )), copy=False)\n",
    "            for div_info in (\n",
    "                dl_info.iloc[s:s + buffer_chunk_size]\n",
    "                for s in range(0, info.shape[0], buffer_chunk_size)\n",
    "            )\n",
    "            for task, parser in zip([\n",
    "                asyncio.create_task(_download(c, semaphore, url))\n",
    "                for url in div_info[\"url\"]\n",
    "            ], div_info[\"parser\"])\n",
    "        ], index=info.index, dtype=str, copy=False)\n",
    "        downloaded.dropna(how=\"all\", inplace=True)\n",
    "        downloaded.rename(columns=_rename, copy=False, inplace=True)\n",
    "        del dl_info\n",
    "\n",
    "    format_spec = _FORMAT_SPEC[output_format]\n",
    "    getattr(\n",
    "        df.join(info, how=\"left\").join(downloaded, how=\"left\"),\n",
    "        f\"to_{format_spec['function_suffix']}\"\n",
    "    )(\n",
    "        Path(f\"{output_name}.{format_spec['suffix']}\"),\n",
    "        header=True,\n",
    "        index=False,\n",
    "        # index=True,\n",
    "        **format_spec[\"extra_args\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-harbor",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "\n",
    "- **PHRASES** (`List[str]`): A list of keywords or phrases to search for. Can be an empty list.\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **INTERVAL** (`Optional[Dict[str, int]]`): The interval of each period, `None` implies the whole period will be searched at once. Reducing the interval will result in more queires have to be made, but it will be useful if the number of results returned exceed the maximum capicity (10000) in one query.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types. I can add pre-check for this variable, but since we assume that all inputs are valid, the check was not added.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query.\n",
    "\n",
    "- **CIKS_PER_QUERY** (`int`): Controls the number of CIKs included in one query. Recommended value is 5, but can be adjusted in case the number of results returned exceed the maximum capacity (10000) in one query.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE**: The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT**: The file format of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regular-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/0000723612/000072361213000018/f10q_agricon33113.htm file will be skipped: (404) Not Found\n",
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n",
      "https://www.sec.gov/Archives/edgar/data/0001385818/000119312515330782/d85936d10k.htm file will be skipped: (404) Not Found\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "_PHRASES = [\"keywords one\", \"keywords two\"]\n",
    "\n",
    "_FILING_TYPES = [\"10-K\", \"10-Q\"]\n",
    "\n",
    "_DATE_START = date.fromisoformat(\"2012-01-01\")\n",
    "_DATE_END = date.fromisoformat(\"2019-12-31\")\n",
    "\n",
    "# _INTERVAL = {\n",
    "#     \"years\": 0,\n",
    "#     \"months\": 1,\n",
    "#     \"weeks\": 0,\n",
    "#     \"days\": 0\n",
    "# }\n",
    "_INTERVAL = None # can be optional\n",
    "\n",
    "# _CIKS = [1961, \"0000003116\"] # accept a plain list of the CIKs\n",
    "_CIKS = Path(\"input_filename.txt\") # accept a file path\n",
    "# _CIKS = 1961 # accept a single CIK as an integer\n",
    "# _CIKS = \"0000003116\" # accept a single CIK as a string\n",
    "# _CIKS = None # can be optional\n",
    "\n",
    "_CIKS_PER_QUERY = 5 # will be ignored if no CIKs is provided\n",
    "\n",
    "_BUFFER_CHUNK_SIZE = 100\n",
    "\n",
    "_OUTPUT_NAME = \"output_filename\"\n",
    "\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "await crawl(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _INTERVAL,\n",
    "    _CIKS,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
