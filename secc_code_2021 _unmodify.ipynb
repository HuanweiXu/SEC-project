{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "musical-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "     active environment : UoA-ABS-RA\n",
      "    active env location : C:\\Users\\laitingsheng\\miniconda3\\envs\\UoA-ABS-RA\n",
      "            shell level : 2\n",
      "       user config file : C:\\Users\\laitingsheng\\.condarc\n",
      " populated config files : C:\\Users\\laitingsheng\\.condarc\n",
      "          conda version : 4.10.3\n",
      "    conda-build version : not installed\n",
      "         python version : 3.8.10.final.0\n",
      "       virtual packages : __cuda=11.4=0\n",
      "                          __win=0=0\n",
      "                          __archspec=1=x86_64\n",
      "       base environment : C:\\Users\\laitingsheng\\miniconda3  (writable)\n",
      "      conda av data dir : C:\\Users\\laitingsheng\\miniconda3\\etc\\conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "                          https://conda.anaconda.org/conda-forge/win-64\n",
      "                          https://conda.anaconda.org/conda-forge/noarch\n",
      "          package cache : C:\\Users\\laitingsheng\\miniconda3\\pkgs\n",
      "                          C:\\Users\\laitingsheng\\.conda\\pkgs\n",
      "                          C:\\Users\\laitingsheng\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\Users\\laitingsheng\\miniconda3\\envs\n",
      "                          C:\\Users\\laitingsheng\\.conda\\envs\n",
      "                          C:\\Users\\laitingsheng\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/4.10.3 requests/2.25.1 CPython/3.8.10 Windows/10 Windows/10.0.22000\n",
      "          administrator : False\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install --yes -c defaults -c conda-forge --update-all python=3.8 aiohttp aiodns bs4 cchardet Faker lxml openpyxl pandas PyPDF2 python-dateutil regex tenacity\n",
    "%conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77361c4-f9e6-47d2-adb9-d6881fac2ed1",
   "metadata": {},
   "source": [
    "# SEC Constants\n",
    "\n",
    "This section has been commented out since it doesn't need to be run frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14f2031-3d0f-4842-9d41-4504fd838ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "headers = {\"User-Agent\":\"a1835057@student.adelaide.edu.au\",'Accept-Encoding':\"text\",\"Host\":\"www.sec.gov\"}\n",
    "\n",
    "async with ClientSession(raise_for_status=True,headers = headers) as c:\n",
    "    async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "        _script = await res.text()\n",
    "\n",
    "    with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"_FORMS = \")\n",
    "        json.dump({\n",
    "            form.pop(\"form\"): form\n",
    "            for form in eval(re.search(\n",
    "                R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                _script,\n",
    "                re.MULTILINE\n",
    "            )[1])\n",
    "        }, f, indent=4)\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.write(\"_LOCATIONS = \")\n",
    "        json.dump(dict(eval(re.search(\n",
    "            R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "            _script,\n",
    "            re.MULTILINE\n",
    "        )[1])), f, indent=4)\n",
    "\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81373d78-1f07-41f5-87be-153c75604e6d",
   "metadata": {},
   "source": [
    "## Common Functions\n",
    "\n",
    "The `chop_periods` function may be replaced by pd.interval_range once the functionality is enhanced in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8ad37e6-80df-40f6-844b-570cc1c7da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from typing import Any, Callable, Dict, Generator, Optional, Tuple\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "def chop_periods(\n",
    "    start_date: date,\n",
    "    end_date: date,\n",
    "    interval: Optional[Dict[str, Any]],\n",
    "    _format: Callable[[date], str]\n",
    ") -> Generator[Tuple[str, str], None, None]:\n",
    "    if interval is None:\n",
    "        yield _format(start_date), _format(end_date)\n",
    "        return\n",
    "\n",
    "    delta = relativedelta(**interval)\n",
    "    # the end date of each period must have an offset of -1 day since the RESTful API is inclusive in both sides\n",
    "    offset = relativedelta(days=1)\n",
    "    next_date = start_date\n",
    "    while (next_date := (curr_date := next_date) + delta) < end_date:\n",
    "        yield _format(curr_date), _format(next_date - offset)\n",
    "    yield _format(curr_date), _format(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a0701-f085-435c-8cda-7b2000ec549f",
   "metadata": {},
   "source": [
    "## Primary Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d279156-4771-48c7-960c-89b6a83cec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry\n",
    "\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import parsers\n",
    "\n",
    "import logging\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "\n",
    "# parse the name and CIK from the result (same as the official JavaScript)\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "\n",
    "_FAKER = Faker()\n",
    "\n",
    "_FORMAT_SPEC = {\n",
    "    \"csv\": {\n",
    "        \"suffix\": \"csv\",\n",
    "        \"function_suffix\": \"csv\",\n",
    "        \"extra_args\": {\n",
    "            \"encoding\": \"utf-8\"\n",
    "        }\n",
    "    },\n",
    "    \"excel\": {\n",
    "        \"suffix\": \"xlsx\",\n",
    "        \"function_suffix\": \"excel\",\n",
    "        \"extra_args\": {\n",
    "            \"sheet_name\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "\n",
    "# Replace by a single space in text\n",
    "_WHITESPACE_REGEX = regex.compile(R\"\\s+\", regex.V1)\n",
    "\n",
    "\n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "\n",
    "def _decode(b: bytes, e: str) -> str:\n",
    "    return b.decode(e)\n",
    "\n",
    "\n",
    "@retry\n",
    "async def _download(client: ClientSession, semaphore: asyncio.Semaphore, url: str) -> Tuple[bytes, str]:\n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            return await res.read(), res.get_encoding()\n",
    "        if res.status in _RETRY_SC:\n",
    "            res.raise_for_status()\n",
    "    _LOGGER.warning(f\"{url} file will be skipped: ({res.status}) {res.reason}\")\n",
    "    return b'', \"ascii\"\n",
    "\n",
    "\n",
    "@retry\n",
    "async def _fetch(\n",
    "    client: ClientSession,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    fixed_query: Dict[str, Any],\n",
    "    ciks: Optional[List[str]],\n",
    "    start_date: str,\n",
    "    end_date: str\n",
    ") -> Tuple[bytes, str]:\n",
    "    async with semaphore, client.post(\"https://efts.sec.gov/LATEST/search-index\", json={\n",
    "        **fixed_query,\n",
    "        \"startdt\": start_date,\n",
    "        \"enddt\": end_date,\n",
    "        \"ciks\": ciks\n",
    "    }) as res:\n",
    "    # async with semaphore, client.request(method='get',url=\"https://efts.sec.gov/LATEST/search-index\",params ={\n",
    "    #     **fixed_query,\n",
    "    #     \"startdt\": start_date,\n",
    "    #     \"enddt\": end_date,\n",
    "    #     \"ciks\": ciks\n",
    "    # }) as res:\n",
    "    \n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            return await res.read(), res.get_encoding()\n",
    "        if res.status in _RETRY_SC:\n",
    "            res.raise_for_status()\n",
    "        _LOGGER.warning(f\"{ciks}-{start_date}-{end_date} query will be skipped: ({res.status}) {res.reason}\")\n",
    "        return b'', \"ascii\"\n",
    "\n",
    "\n",
    "def _iso(d: date):\n",
    "    return d.isoformat()\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]):\n",
    "    if not hit:\n",
    "        return \" \",\" \"\n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), loc, _LOCATIONS.get(code, code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"inc_states\"],\n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        )\n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], dtype=str, copy=False)\n",
    "\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows[\"cik\"]\n",
    "\n",
    "    info = pd.Series({\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks[ciks.notnull()].iloc[-1], adsh.replace('-', ''), filename),\n",
    "        \"parser\": getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    }, dtype=object, copy=False)\n",
    "    return rows, info\n",
    "\n",
    "\n",
    "def _rename(index):\n",
    "    return f\"paragraph{index + 1}\"\n",
    "\n",
    "\n",
    "def _unwrap(hits: Dict[str, Any]):\n",
    "    print(hits)\n",
    "    total_hits = hits[\"total\"]\n",
    "    if total_hits[\"relation\"] == \"gte\":\n",
    "        _LOGGER.warning(f\"The query returns a result exceeding the 10k limit\")\n",
    "    #print(hits[\"hits\"])\n",
    "    #hits =  json.loads(hits.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits[\"hits\"]\n",
    "    #return hits\n",
    "\n",
    "\n",
    "def chop_ciks(\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int\n",
    ") -> Generator[Optional[List[str]], None, None]:\n",
    "    # defaults to None\n",
    "    _ciks: Optional[List[str]] = None\n",
    "    # if the provided parameter is a Path, read the CIKs from the file\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "\n",
    "    if _ciks:\n",
    "        for i in range(0, len(_ciks), ciks_per_query):\n",
    "            yield _ciks[i:i + ciks_per_query]\n",
    "    else:\n",
    "        yield None\n",
    "\n",
    "\n",
    "async def crawl(\n",
    "    phrases: List[str],\n",
    "    filing_types: List[str],\n",
    "    start_date: date,\n",
    "    end_date: date,\n",
    "    interval: Optional[Dict[str, int]],\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int,\n",
    "    buffer_chunk_size: int,\n",
    "    output_name: str,\n",
    "    output_format: str,\n",
    "    headers:dict\n",
    "):\n",
    "    fixed_query: Dict[str, Any] = {\n",
    "        \"q\": \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases),\n",
    "        \"category\": \"custom\",\n",
    "        \"forms\": filing_types,\n",
    "        \"dateRange\": \"custom\"\n",
    "    }\n",
    "\n",
    "    phrases_regex = regex.compile(\n",
    "        \"|\".join(f\"(?:{phrase})\" for phrase in map(regex.escape, phrases)),\n",
    "        regex.V1 | regex.IGNORECASE\n",
    "    )\n",
    "\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    async with ClientSession(headers=headers) as c:\n",
    "        dfs, infos = zip(*[\n",
    "            _parse_hit(hit) \n",
    "            for task in [\n",
    "                asyncio.create_task(_fetch(c, semaphore, fixed_query, ciks, *period))\n",
    "                for ciks in chop_ciks(ciks, ciks_per_query)\n",
    "                for period in chop_periods(start_date, end_date, interval, _iso)\n",
    "            ]\n",
    "            for hit in _unwrap(json.loads(_decode(*await task))[\"hits\"]) if _parse_hit(hit)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        df = pd.concat(dfs, ignore_index=True, copy=False)\n",
    "        df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "        df.set_index(keys=\"id\", inplace=True, verify_integrity=False)\n",
    "        del dfs\n",
    "\n",
    "        info = pd.DataFrame(infos, dtype=object, copy=False)\n",
    "        info.dropna(subset=[\"parser\"], inplace=True)\n",
    "        info.drop_duplicates(subset=\"id\", inplace=True, ignore_index=True)\n",
    "        info.set_index(keys=\"id\", inplace=True)\n",
    "        del infos\n",
    "\n",
    "        dl_info = info[[\"url\", \"parser\"]]\n",
    "        del info[\"parser\"]\n",
    "        downloaded = pd.DataFrame([\n",
    "            pd.Series(filter(phrases_regex.search, (\n",
    "                _CC_REGEX.sub(\"\", _WHITESPACE_REGEX.sub(\" \", s).strip())\n",
    "                for s in parser(*await task).split(\"\\n\\n\")\n",
    "            )), copy=False)\n",
    "            for div_info in (\n",
    "                dl_info.iloc[s:s + buffer_chunk_size]\n",
    "                for s in range(0, info.shape[0], buffer_chunk_size)\n",
    "            )\n",
    "            for task, parser in zip([\n",
    "                asyncio.create_task(_download(c, semaphore, url))\n",
    "                for url in div_info[\"url\"]\n",
    "            ], div_info[\"parser\"])\n",
    "        ], index=info.index, dtype=str, copy=False)\n",
    "        downloaded.dropna(how=\"all\", inplace=True)\n",
    "        downloaded.rename(columns=_rename, copy=False, inplace=True)\n",
    "        del dl_info\n",
    "\n",
    "    format_spec = _FORMAT_SPEC[output_format]\n",
    "    getattr(\n",
    "        df.join(info, how=\"left\").join(downloaded, how=\"left\"),\n",
    "        f\"to_{format_spec['function_suffix']}\"\n",
    "    )(\n",
    "        Path(f\"{output_name}.{format_spec['suffix']}\"),\n",
    "        header=True,\n",
    "        index=False,\n",
    "        # index=True,\n",
    "        **format_spec[\"extra_args\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-harbor",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "\n",
    "- **PHRASES** (`List[str]`): A list of keywords or phrases to search for. Can be an empty list.\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **INTERVAL** (`Optional[Dict[str, int]]`): The interval of each period, `None` implies the whole period will be searched at once. Reducing the interval will result in more queires have to be made, but it will be useful if the number of results returned exceed the maximum capicity (10000) in one query.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types. I can add pre-check for this variable, but since we assume that all inputs are valid, the check was not added.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query.\n",
    "\n",
    "- **CIKS_PER_QUERY** (`int`): Controls the number of CIKs included in one query. Recommended value is 5, but can be adjusted in case the number of results returned exceed the maximum capacity (10000) in one query.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE**: The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT**: The file format of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "regular-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': {'value': 1, 'relation': 'eq'}, 'max_score': 2.842336, 'hits': [{'_index': 'edgar_file', '_type': '_doc', '_id': '0001193125-23-085782:d261382dars.pdf', '_score': 2.842336, '_source': {'ciks': ['0000006201'], 'period_ending': '2022-12-31', 'root_form': 'ARS', 'file_num': ['001-08400'], 'display_names': ['American Airlines Group Inc.  (AAL)  (CIK 0000006201)'], 'xsl': None, 'sequence': '1', 'file_date': '2023-03-30', 'biz_states': ['TX'], 'sics': ['4512'], 'form': 'ARS', 'adsh': '0001193125-23-085782', 'film_num': ['23781867'], 'biz_locations': ['Fort Worth, TX'], 'file_type': 'ARS', 'file_description': 'ARS', 'inc_states': ['DE'], 'items': []}}]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PyPdfError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/d/summer_research/parsers.py:25\u001b[0m, in \u001b[0;36m_parse_pdf\u001b[0;34m(content, _)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content\u001b[38;5;241m.\u001b[39mdecode(encoding)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_pdf\u001b[39m(content: \u001b[38;5;28mbytes\u001b[39m, _: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PdfFileReader' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# _OUTPUT_FORMAT = \"csv\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m _OUTPUT_FORMAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m crawl(\n\u001b[1;32m     36\u001b[0m     _PHRASES,\n\u001b[1;32m     37\u001b[0m     _FILING_TYPES,\n\u001b[1;32m     38\u001b[0m     _DATE_START,\n\u001b[1;32m     39\u001b[0m     _DATE_END,\n\u001b[1;32m     40\u001b[0m     _INTERVAL,\n\u001b[1;32m     41\u001b[0m     _CIKS,\n\u001b[1;32m     42\u001b[0m     _CIKS_PER_QUERY,\n\u001b[1;32m     43\u001b[0m     _BUFFER_CHUNK_SIZE,\n\u001b[1;32m     44\u001b[0m     _OUTPUT_NAME,\n\u001b[1;32m     45\u001b[0m     _OUTPUT_FORMAT,\n\u001b[1;32m     46\u001b[0m     headers\n\u001b[1;32m     47\u001b[0m )\n",
      "Cell \u001b[0;32mIn[68], line 277\u001b[0m, in \u001b[0;36mcrawl\u001b[0;34m(phrases, filing_types, start_date, end_date, interval, ciks, ciks_per_query, buffer_chunk_size, output_name, output_format, headers)\u001b[0m\n\u001b[1;32m    275\u001b[0m dl_info \u001b[38;5;241m=\u001b[39m info[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 277\u001b[0m downloaded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m    278\u001b[0m     pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mfilter\u001b[39m(phrases_regex\u001b[38;5;241m.\u001b[39msearch, (\n\u001b[1;32m    279\u001b[0m         _CC_REGEX\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, _WHITESPACE_REGEX\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, s)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m parser(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m task)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m     )), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m div_info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    283\u001b[0m         dl_info\u001b[38;5;241m.\u001b[39miloc[s:s \u001b[38;5;241m+\u001b[39m buffer_chunk_size]\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, info\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], buffer_chunk_size)\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task, parser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\n\u001b[1;32m    287\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mcreate_task(_download(c, semaphore, url))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m div_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    289\u001b[0m     ], div_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    290\u001b[0m ], index\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    291\u001b[0m downloaded\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    292\u001b[0m downloaded\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m_rename, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[68], line 280\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    275\u001b[0m dl_info \u001b[38;5;241m=\u001b[39m info[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    277\u001b[0m downloaded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m    278\u001b[0m     pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mfilter\u001b[39m(phrases_regex\u001b[38;5;241m.\u001b[39msearch, (\n\u001b[1;32m    279\u001b[0m         _CC_REGEX\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, _WHITESPACE_REGEX\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, s)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m     )), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m div_info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    283\u001b[0m         dl_info\u001b[38;5;241m.\u001b[39miloc[s:s \u001b[38;5;241m+\u001b[39m buffer_chunk_size]\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, info\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], buffer_chunk_size)\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task, parser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\n\u001b[1;32m    287\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mcreate_task(_download(c, semaphore, url))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m div_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    289\u001b[0m     ], div_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    290\u001b[0m ], index\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    291\u001b[0m downloaded\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    292\u001b[0m downloaded\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m_rename, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/d/summer_research/parsers.py:26\u001b[0m, in \u001b[0;36m_parse_pdf\u001b[0;34m(content, _)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_pdf\u001b[39m(content: \u001b[38;5;28mbytes\u001b[39m, _: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(page\u001b[38;5;241m.\u001b[39mextractText() \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m PdfFileReader(BytesIO(content))\u001b[38;5;241m.\u001b[39mpages)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m PyPdfError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PyPdfError' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from pathlib import Path\n",
    "headers = {\"User-Agent\":\"a1835057@student.adelaide.edu.au\"}#,'Accept-Encoding':\"text\",\"Host\":\"www.sec.gov\"\n",
    "\n",
    "_PHRASES = [\"data breach\", \"cyber security\"]\n",
    "\n",
    "_FILING_TYPES = [\"\"]#\"10-K\", \"10-Q\"\n",
    "\n",
    "_DATE_START = date.fromisoformat(\"2001-12-01\")\n",
    "_DATE_END = date.fromisoformat(\"2023-12-31\")\n",
    "\n",
    "# _INTERVAL = {\n",
    "#     \"years\": 0,\n",
    "#     \"months\": 1,\n",
    "#     \"weeks\": 0,\n",
    "#     \"days\": 0\n",
    "# }\n",
    "_INTERVAL = None # can be optional\n",
    "\n",
    "# _CIKS = [1961, \"0000003116\"] # accept a plain list of the CIKs\n",
    "#_CIKS = Path(\"sample_input_file1.txt\") # accept a file path\n",
    "# _CIKS = 1961 # accept a single CIK as an integer\n",
    "_CIKS = \"0000006201\" # accept a single CIK as a string\n",
    "# _CIKS = None # can be optional\n",
    "\n",
    "_CIKS_PER_QUERY = 5 # will be ignored if no CIKs is provided\n",
    "\n",
    "_BUFFER_CHUNK_SIZE = 100\n",
    "\n",
    "_OUTPUT_NAME = \"20231224.xlsx\"\n",
    "\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "await crawl(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _INTERVAL,\n",
    "    _CIKS,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT,\n",
    "    headers\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
