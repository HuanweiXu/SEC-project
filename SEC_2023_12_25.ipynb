{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry\n",
    "import time\n",
    "from tqdm import *\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "\n",
    "@retry\n",
    "async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "    q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "    #forms = \" \".join(form for  form in forms)\n",
    "    data = {'q':q,\n",
    "            'startdt':start,\n",
    "            'enddt':end,\n",
    "            'ciks':cik,\n",
    "            'dataRange':'custom',\n",
    "            'category':'custom',\n",
    "            'forms':forms}\n",
    "    url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    async with semaphore,client.request(method='get',url=url,params = data) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 200:\n",
    "            result = await res.read()\n",
    "            fetch_bar.update(1)\n",
    "            return result#await res.json()\n",
    "        raise ValueError(f\"Status Code = {res.status}\")\n",
    "\n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "@retry\n",
    "async def _download(semaphore: asyncio.Semaphore, row_index, df,client,keywords,download_bar):\n",
    "    url = df.loc[row_index,\"url\"]\n",
    "    ext = df.loc[row_index,\"file_ext\"]\n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            html = await res.read()\n",
    "        if ext == \"htm\":\n",
    "            paragraphs = extract_html(html,keywords)\n",
    "        elif ext == \"pdf\":\n",
    "            paragraphs = extract_pdf(html,keywords)\n",
    "        elif ext == \"txt\":\n",
    "            paragraphs = extract_txt(html,keywords)\n",
    "        else:\n",
    "            tem = f\"paragrah{1}\"\n",
    "            df.loc[row_index,tem] = 'Unknow extension'\n",
    "            res.raise_for_status()\n",
    "            return download_bar.update(1)\n",
    "        if paragraphs:\n",
    "            for num in range(len(paragraphs)):\n",
    "                tem = f\"paragrah{num + 1}\"\n",
    "                df.loc[row_index,tem] = str(paragraphs[num]).strip()\n",
    "            return download_bar.update(1)\n",
    "        if res.status in _RETRY_SC:\n",
    "            _LOGGER.warning(f\"{url} file will be skipped: ({res.status}) {res.reason}\")\n",
    "            tem = f\"paragrah{1}\"\n",
    "            df.loc[row_index,tem] = 'Download skipped'\n",
    "            return download_bar.update(1)\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]): \n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    \n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), str(loc).split(\",\")[0], _LOCATIONS.get(code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"biz_states\"], #source[\"inc_states\"] if source[\"inc_states\"] else \n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        ) \n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], copy=False)#, dtype=str\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows.loc[0,\"cik\"]\n",
    "    info = pd.DataFrame({\n",
    "        \"entity_name\":rows['entity_name'],\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks, adsh.replace('-', ''), filename),\n",
    "        \"parser\": None\n",
    "    },copy=False,dtype=str)\n",
    "    \n",
    "    result = pd.merge(rows,info,how=\"left\",on=\"id\")\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def extract_html(html, keywords):\n",
    "    matching_paragraphs = [para.replace(\"\\n\",\" \").replace(\"\\xa0\",\" \").strip() \n",
    "                           for chunks in BeautifulSoup(html,\"lxml\",from_encoding='utf-8').get_text().split(\"\\n\\n\")\n",
    "                           for chunk in chunks.split('\\n\\xa0\\n') \n",
    "                           for para in chunk.split(\"  \")\n",
    "                           if any(keyword in para.lower() \n",
    "                                  for keyword in keywords)]\n",
    "    return list(set(matching_paragraphs))\n",
    "\n",
    "def extract_pdf(fetch,keywords):\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    #pattern = regex.compile(r'(\\n\\W*\\n)', regex.V1)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=\"pdf\")\n",
    "    #line_separator_pattern = regex.compile(r'(\\n\\W*?\\n)')\n",
    "\n",
    "    total = [para.replace(\"\\n\",\" \") for page in range(pdf_document.page_count) \n",
    "             #for para in regex.split(line_separator, pdf_document[page].get_text(\"text\")) \n",
    "            for lists in pdf_document[page].get_text(\"blocks\") \n",
    "            for  para in lists \n",
    "            if any(keyword in str(para) for keyword in keywords) ]\n",
    "    return total\n",
    "\n",
    "def extract_txt(bytes,keywords):\n",
    "    txt = bytes.decode('utf-8').split('\\n\\n')\n",
    "    txt = [i.replace(\"\\n\",\" \")for i in txt]\n",
    "    matching_paragraphs = [paragraph for paragraph in txt if any(keyword in paragraph.lower() for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "def CIK(file):\n",
    "    with open(file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "            return _ciks\n",
    "        except IOError as e:\n",
    "                raise ValueError(f\"{file} is not a valid file\") from e\n",
    "    # if _ciks:\n",
    "    #     for i in range(0, len(_ciks), ciks_per_query):\n",
    "    #         yield _ciks[i:i + ciks_per_query]\n",
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    # if total_hits == 0:\n",
    "    #     return None\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits\n",
    "\n",
    "\n",
    "\n",
    "async def main(_PHRASES,_FILING_TYPES,_DATE_START,_DATE_END,_CIKS_PER_QUERY, _CIKS,_buffer_chunk_size,df,headers,_OUTPUT_NAME,_OUTPUT_FORMAT):\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    if _FILING_TYPES == []:\n",
    "        _FILING_TYPES = [\"\"]\n",
    "    #-------------- Crawl --------\n",
    "    async with ClientSession(raise_for_status=True, headers=headers) as client :\n",
    "        #------Fetch--------\n",
    "        total = len(_CIKS) * len(_FILING_TYPES)\n",
    "        with tqdm(        \n",
    "            total=total) as fetch_bar:\n",
    "            print(\"Starting fetch...\")\n",
    "            fetch_tasks = [\n",
    "                    asyncio.create_task(fetch(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        phrases=_PHRASES,\n",
    "                        cik=cik,\n",
    "                        start=_DATE_START,\n",
    "                        end=_DATE_END,\n",
    "                        forms=form,\n",
    "                        fetch_bar=fetch_bar\n",
    "                    ))\n",
    "                    for  form in _FILING_TYPES for cik in _CIKS\n",
    "                ]\n",
    "            fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "            df = pd.concat([df] + \n",
    "                        [_parse_hit(hit) for data in fetched_data for hit in decode(data)],\n",
    "                        ignore_index=True)\n",
    "            df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "        #-----Docs download-------\n",
    "        print(f\"fetch completed and collected [{df.shape[0]}] of docs,starting download docs..\")\n",
    "        total = df.shape[0]\n",
    "        if total == 0:\n",
    "            print(\"Fetch completed with 0 result. Now existing.\")\n",
    "            return\n",
    "        with tqdm(total=total) as download_bar:\n",
    "            for index in range(0,total,_buffer_chunk_size):\n",
    "                index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "                download_tasks = [\n",
    "                    asyncio.create_task(_download(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        df = df,\n",
    "                        row_index=row,\n",
    "                        download_bar=download_bar,\n",
    "                        keywords=_PHRASES)\n",
    "                    )for row in index_range\n",
    "                    ]\n",
    "                downloaded = await asyncio.gather(*download_tasks)\n",
    "                \n",
    "\n",
    "\n",
    "    del df[\"parser\"] \n",
    "    del df['id']\n",
    "    if _OUTPUT_FORMAT in [\"excel\",\"xlsx\"]:\n",
    "        df.to_excel(f\"{_OUTPUT_NAME}.xlsx\")\n",
    "    elif _OUTPUT_FORMAT.lower() == \"csv\":      \n",
    "        df.to_csv(f\"{_OUTPUT_NAME}.csv\") \n",
    "    print(f\"Data have been export at {os.getcwd()}\") \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "T0 = time.time()\n",
    "df = pd.DataFrame()\n",
    "headers = {\"User-Agent\":\"a1835057@student.adelaide.edu.au\"}\n",
    "_PHRASES = [\"data breach\", \"cyber security\"]\n",
    "_FILING_TYPES = [\"\"]#,\n",
    "#_FILING_TYPES = [\"\"10-K\",\"10-Q\"\"]\n",
    "_DATE_START = \"2001-01-01\"\n",
    "_DATE_END = \"2023-12-12\"\n",
    "_CIKS_PER_QUERY = 10\n",
    "_BUFFER_CHUNK_SIZE = 200\n",
    "#_CIKS =  [\"0001653482\"] #Input from a list or a path,,\"0001653481\"\n",
    "_CIKS = CIK(Path(\"sample_input_file.txt\"))[:200] ####Please mannually select N here for Testing for the first N CIKs###\n",
    "_OUTPUT_NAME = \"20231224.xlsx\"\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "\n",
    "df = await main(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _CIKS,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    df,\n",
    "    headers,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT\n",
    ")\n",
    " \n",
    "END = time.time()\n",
    "print(\"--\"*10,\n",
    "      f\"All tasks completed! Time Cost:{round((END-T0)/60,1)} minutes \",sep='\\n')#pymupdf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
