{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb0d25c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "     active environment : base\n",
      "    active env location : C:\\Users\\drche\\Software\\anaconda3\n",
      "            shell level : 1\n",
      "       user config file : C:\\Users\\drche\\.condarc\n",
      " populated config files : \n",
      "          conda version : 23.11.0\n",
      "    conda-build version : 3.28.2\n",
      "         python version : 3.8.18.final.0\n",
      "                 solver : libmamba (default)\n",
      "       virtual packages : __archspec=1=x86_64\n",
      "                          __conda=23.11.0=0\n",
      "                          __cuda=12.0=0\n",
      "                          __win=0=0\n",
      "       base environment : C:\\Users\\drche\\Software\\anaconda3  (writable)\n",
      "      conda av data dir : C:\\Users\\drche\\Software\\anaconda3\\etc\\conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "          package cache : C:\\Users\\drche\\Software\\anaconda3\\pkgs\n",
      "                          C:\\Users\\drche\\.conda\\pkgs\n",
      "                          C:\\Users\\drche\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\Users\\drche\\Software\\anaconda3\\envs\n",
      "                          C:\\Users\\drche\\.conda\\envs\n",
      "                          C:\\Users\\drche\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/23.11.0 requests/2.31.0 CPython/3.8.18 Windows/10 Windows/10.0.22621 solver/libmamba conda-libmamba-solver/23.12.0 libmambapy/1.5.3 aau/0.4.3 c/9xLjz6Vo2eoT2nx_7Xzo6g s/x8vth7RRL1S2h-zjGMercQ e/kKVPaga4F8aG30NbhZ3lTw\n",
      "          administrator : False\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install --yes -c defaults -c conda-forge --update-all python=3.8 aiohttp aiodns bs4 cchardet Faker lxml openpyxl pandas PyPDF2 python-dateutil regex tenacity tqdm requests\n",
    "%conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ea0fd",
   "metadata": {},
   "source": [
    "# SEC Constants\n",
    "\n",
    "This section has been commented out since it doesn't need to be run frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0beacd52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientResponseError",
     "evalue": "403, message='Forbidden', url=URL('https://www.sec.gov/edgar/search/js/edgar_full_text_search.js')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16816\\2971673397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mClientSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0m_script\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Software\\anaconda3\\lib\\site-packages\\aiohttp\\client.py\u001b[0m in \u001b[0;36m__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m__aenter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_RetType\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Software\\anaconda3\\lib\\site-packages\\aiohttp\\client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mawait\u001b[0m \u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mraise_for_status\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m                 \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;31m# register connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Software\\anaconda3\\lib\\site-packages\\aiohttp\\client_reqrep.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m             raise ClientResponseError(\n\u001b[0m\u001b[0;32m   1063\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mClientResponseError\u001b[0m: 403, message='Forbidden', url=URL('https://www.sec.gov/edgar/search/js/edgar_full_text_search.js')"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async with ClientSession(raise_for_status=True) as c:\n",
    "    async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "        _script = await res.text()\n",
    "\n",
    "    with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"_FORMS = \")\n",
    "        json.dump({\n",
    "            form.pop(\"form\"): form\n",
    "            for form in eval(re.search(\n",
    "                R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                _script,\n",
    "                re.MULTILINE\n",
    "            )[1])\n",
    "        }, f, indent=4)\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.write(\"_LOCATIONS = \")\n",
    "        json.dump(dict(eval(re.search(\n",
    "            R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "            _script,\n",
    "            re.MULTILINE\n",
    "        )[1])), f, indent=4)\n",
    "\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af13844",
   "metadata": {},
   "source": [
    "## Common Functions\n",
    "\n",
    "The `chop_periods` function may be replaced by pd.interval_range once the functionality is enhanced in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from typing import Any, Callable, Dict, Generator, Optional, Tuple\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "def chop_periods(\n",
    "    start_date: date,\n",
    "    end_date: date,\n",
    "    interval: Optional[Dict[str, Any]],\n",
    "    _format: Callable[[date], str]\n",
    ") -> Generator[Tuple[str, str], None, None]:\n",
    "    if interval is None:\n",
    "        yield _format(start_date), _format(end_date)\n",
    "        return\n",
    "\n",
    "    delta = relativedelta(**interval)\n",
    "    # the end date of each period must have an offset of -1 day since the RESTful API is inclusive in both sides\n",
    "    offset = relativedelta(days=1)\n",
    "    next_date = start_date\n",
    "    while (next_date := (curr_date := next_date) + delta) < end_date:\n",
    "        yield _format(curr_date), _format(next_date - offset)\n",
    "    yield _format(curr_date), _format(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf2796",
   "metadata": {},
   "source": [
    "## Primary Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c92b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from datetime import date\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry\n",
    "\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import parsers\n",
    "\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "\n",
    "# parse the name and CIK from the result (same as the official JavaScript)\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "\n",
    "_FAKER = Faker()\n",
    "\n",
    "_FORMAT_SPEC = {\n",
    "    \"csv\": {\n",
    "        \"suffix\": \"csv\",\n",
    "        \"function_suffix\": \"csv\",\n",
    "        \"extra_args\": {\n",
    "            \"encoding\": \"utf-8\"\n",
    "        }\n",
    "    },\n",
    "    \"excel\": {\n",
    "        \"suffix\": \"xlsx\",\n",
    "        \"function_suffix\": \"excel\",\n",
    "        \"extra_args\": {\n",
    "            \"sheet_name\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "\n",
    "# Replace by a single space in text\n",
    "_WHITESPACE_REGEX = regex.compile(R\"\\s+\", regex.V1)\n",
    "\n",
    "\n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "\n",
    "def _decode(b: bytes, e: str) -> str:\n",
    "    return b.decode(e)\n",
    "\n",
    "\n",
    "@retry\n",
    "async def _download(client: ClientSession, semaphore: asyncio.Semaphore, url: str, user_agent: str) -> Tuple[bytes, str]:\n",
    "    async with semaphore, client.get(url, headers={\n",
    "        'User-Agent': user_agent,\n",
    "        'Accept-Encoding': 'text',\n",
    "    }) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            return await res.read(), res.get_encoding()\n",
    "        if res.status in _RETRY_SC:\n",
    "            res.raise_for_status()\n",
    "    _LOGGER.warning(f\"{url} file will be skipped: ({res.status}) {res.reason}\")\n",
    "    return b'', \"ascii\"\n",
    "\n",
    "\n",
    "@retry\n",
    "async def _fetch(\n",
    "    client: ClientSession,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    fixed_query: Dict[str, Any],\n",
    "    ciks: Optional[List[str]],\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    page_iterator: Dict[str, Any]\n",
    ") -> Tuple[bytes, str]:\n",
    "    req_body = {\n",
    "        **fixed_query,\n",
    "        **page_iterator,\n",
    "        \"startdt\": start_date,\n",
    "        \"enddt\": end_date,\n",
    "        \"ciks\": ciks\n",
    "    }\n",
    "#     print(req_body)\n",
    "    async with semaphore, client.post(\"https://efts.sec.gov/LATEST/search-index\", json=req_body) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.ok:\n",
    "            return await res.read(), res.get_encoding()\n",
    "        if res.status in _RETRY_SC:\n",
    "            res.raise_for_status()\n",
    "        _LOGGER.warning(f\"{ciks}-{start_date}-{end_date} query will be skipped: ({res.status}) {res.reason}\")\n",
    "        return b'', \"ascii\"\n",
    "\n",
    "\n",
    "def _iso(d: date):\n",
    "    return d.isoformat()\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]):\n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), loc, _LOCATIONS.get(code, code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"inc_states\"],\n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        )\n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], dtype=str, copy=False)\n",
    "\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows[\"cik\"]\n",
    "\n",
    "    info = pd.Series({\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks[ciks.notnull()].iloc[-1], adsh.replace('-', ''), filename),\n",
    "        \"parser\": getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    }, dtype=object, copy=False)\n",
    "    return rows, info\n",
    "\n",
    "\n",
    "def _rename(index):\n",
    "    return f\"paragraph{index + 1}\"\n",
    "\n",
    "\n",
    "def _unwrap(hits: Dict[str, Any], no_ciks):\n",
    "    total_hits = hits[\"total\"]\n",
    "    if not no_ciks:\n",
    "        if total_hits[\"relation\"] == \"gte\":\n",
    "            _LOGGER.warning(f\"The query returns a result exceeding the 10k limit\")\n",
    "    return hits[\"hits\"]\n",
    "\n",
    "def _check_limit(hits: Dict[str, Any]):\n",
    "    total_hits = hits[\"hits\"][\"total\"]\n",
    "    print(\"Total fetchable results: \" + str(total_hits[\"value\"]))\n",
    "    return total_hits[\"value\"]\n",
    "\n",
    "def _split_to_page_and_from(count):\n",
    "    number_of_pages = count / _EDGAR_RESULTS_PER_PAGE\n",
    "    from_count = 0\n",
    "    split_list = []\n",
    "    print(\"---Fetching results over a range of pages---\")\n",
    "    print(\"Total number of pages: \" + str(int(number_of_pages)))\n",
    "    print(\"Number of results per page: \" + str(_EDGAR_RESULTS_PER_PAGE))\n",
    "    for i in range(1, int(number_of_pages + 1)):\n",
    "        split_list.append({\n",
    "            \"page\": i,\n",
    "            \"from\": from_count\n",
    "        })\n",
    "        from_count = from_count + _EDGAR_RESULTS_PER_PAGE\n",
    "        \n",
    "    return split_list\n",
    "        \n",
    "\n",
    "def _mock_request(fixed_query, start_date, end_date):\n",
    "    data = {\n",
    "        **fixed_query,\n",
    "        \"startdt\": _iso(start_date),\n",
    "        \"enddt\": _iso(end_date),\n",
    "        \"ciks\": None,\n",
    "    }\n",
    "    response = requests.post(\n",
    "        \"https://efts.sec.gov/LATEST/search-index\",\n",
    "        data=json.dumps(data)\n",
    "    )\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def chop_ciks(\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int\n",
    ") -> Generator[Optional[List[str]], None, None]:\n",
    "    # defaults to None\n",
    "    _ciks: Optional[List[str]] = None\n",
    "    # if the provided parameter is a Path, read the CIKs from the file\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "\n",
    "    if _ciks:\n",
    "        for i in range(0, len(_ciks), ciks_per_query):\n",
    "            yield _ciks[i:i + ciks_per_query]\n",
    "    else:\n",
    "        yield None\n",
    "\n",
    "\n",
    "async def crawl(\n",
    "    phrases: List[str],\n",
    "    filing_types: List[str],\n",
    "    start_date: date,\n",
    "    end_date: date,\n",
    "    interval: Optional[Dict[str, int]],\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int,\n",
    "    buffer_chunk_size: int,\n",
    "    output_name: str,\n",
    "    output_format: str,\n",
    "    user_agent: str\n",
    "):\n",
    "    fixed_query: Dict[str, Any] = {\n",
    "        \"q\": \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases),\n",
    "        \"category\": \"custom\",\n",
    "        \"forms\": filing_types,\n",
    "        \"dateRange\": \"custom\"\n",
    "    }\n",
    "\n",
    "    phrases_regex = regex.compile(\n",
    "        \"|\".join(f\"(?:{phrase})\" for phrase in map(regex.escape, phrases)),\n",
    "        regex.V1 | regex.IGNORECASE\n",
    "    )\n",
    "\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    async with ClientSession() as c:\n",
    "        print(\"Fetching search query results...\")\n",
    "        dfs = None\n",
    "        infos = None\n",
    "        print(\"\\n\")\n",
    "        if ciks:\n",
    "            dfs, infos = zip(*[\n",
    "                _parse_hit(hit)\n",
    "                for task in tqdm([\n",
    "                    asyncio.create_task(_fetch(c, semaphore, fixed_query, ciks, *period, {}))\n",
    "                    for ciks in chop_ciks(ciks, ciks_per_query)\n",
    "                    for period in chop_periods(start_date, end_date, interval, _iso)\n",
    "                ])\n",
    "                for hit in _unwrap(json.loads(_decode(*await task))[\"hits\"], False)\n",
    "            ])\n",
    "        else:\n",
    "            mock_hit = _mock_request(fixed_query, start_date, end_date)\n",
    "            total = _check_limit(mock_hit)\n",
    "            pages = _split_to_page_and_from(total)\n",
    "            print(\"\\n\")\n",
    "            dfs, infos = zip(*[\n",
    "                _parse_hit(hit)\n",
    "                for task in tqdm([\n",
    "                    asyncio.create_task(_fetch(c, semaphore, fixed_query, ciks, *period, page))\n",
    "                    for page in pages\n",
    "                    for period in chop_periods(start_date, end_date, interval, _iso)\n",
    "                ])\n",
    "                for hit in _unwrap(json.loads(_decode(*await task))[\"hits\"], True)\n",
    "            ])\n",
    "        print(\"Fetching search query results complete\")\n",
    "\n",
    "        df = pd.concat(dfs, ignore_index=True, copy=False)\n",
    "        df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "        df.set_index(keys=\"id\", inplace=True, verify_integrity=False)\n",
    "        del dfs\n",
    "\n",
    "        info = pd.DataFrame(infos, dtype=object, copy=False)\n",
    "        info.dropna(subset=[\"parser\"], inplace=True)\n",
    "        info.drop_duplicates(subset=\"id\", inplace=True, ignore_index=True)\n",
    "        info.set_index(keys=\"id\", inplace=True)\n",
    "        del infos\n",
    "\n",
    "        dl_info = info[[\"url\", \"parser\"]]\n",
    "        del info[\"parser\"]\n",
    "        \n",
    "        print(\"Queuing tasks for download of filings...\")\n",
    "        print(\"\\n\")\n",
    "        downloaded = pd.DataFrame([\n",
    "            pd.Series(filter(phrases_regex.search, (\n",
    "                _CC_REGEX.sub(\"\", _WHITESPACE_REGEX.sub(\" \", s).strip())\n",
    "                for s in parser(*await task).split(\"\\n\\n\")\n",
    "            )), copy=False)\n",
    "            for div_info in (\n",
    "                dl_info.iloc[s:s + buffer_chunk_size]\n",
    "                for s in range(0, info.shape[0], buffer_chunk_size)\n",
    "            )\n",
    "            for task, parser in tqdm(zip([\n",
    "                asyncio.create_task(_download(c, semaphore, url, user_agent))\n",
    "#                 await _download(c, semaphore, url, user_agent)\n",
    "                for url in div_info[\"url\"]\n",
    "            ], div_info[\"parser\"]))\n",
    "        ], index=info.index, dtype=str, copy=False)\n",
    "        downloaded.dropna(how=\"all\", inplace=True)\n",
    "        downloaded.rename(columns=_rename, copy=False, inplace=True)\n",
    "        del dl_info\n",
    "\n",
    "    format_spec = _FORMAT_SPEC[output_format]\n",
    "    getattr(\n",
    "        df.join(info, how=\"left\").join(downloaded, how=\"left\"),\n",
    "        f\"to_{format_spec['function_suffix']}\"\n",
    "    )(\n",
    "        Path(f\"{output_name}.{format_spec['suffix']}\"),\n",
    "        header=True,\n",
    "        index=False,\n",
    "        # index=True,\n",
    "        **format_spec[\"extra_args\"]\n",
    "    )\n",
    "    print(\"Queuing complete, please wait for the tasks to complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a5362",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "\n",
    "- **PHRASES** (`List[str]`): A list of keywords or phrases to search for. Can be an empty list.\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **INTERVAL** (`Optional[Dict[str, int]]`): The interval of each period, `None` implies the whole period will be searched at once. Reducing the interval will result in more queires have to be made, but it will be useful if the number of results returned exceed the maximum capicity (10000) in one query.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types. I can add pre-check for this variable, but since we assume that all inputs are valid, the check was not added.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query.\n",
    "\n",
    "- **CIKS_PER_QUERY** (`int`): Controls the number of CIKs included in one query. Recommended value is 5, but can be adjusted in case the number of results returned exceed the maximum capacity (10000) in one query.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE**: The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT**: The file format of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a38ee2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching search query results...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1933/1933 [05:13<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching search query results complete\n",
      "Queuing tasks for download of filings...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:12,  1.38it/s]\n",
      "100it [01:49,  1.09s/it]\n",
      "100it [01:38,  1.02it/s]\n",
      "100it [01:00,  1.65it/s]\n",
      "100it [01:31,  1.09it/s]\n",
      "93it [01:33,  1.03it/s]https://www.sec.gov/Archives/edgar/data/0000723612/000072361213000018/f10q_agricon33113.htm file will be skipped: (404) Not Found\n",
      "100it [01:36,  1.04it/s]\n",
      "100it [01:43,  1.04s/it]\n",
      "100it [01:22,  1.21it/s]\n",
      "100it [01:21,  1.22it/s]\n",
      "100it [02:05,  1.25s/it]\n",
      "100it [01:29,  1.11it/s]\n",
      "100it [01:18,  1.27it/s]\n",
      "100it [01:31,  1.09it/s]\n",
      "100it [02:00,  1.21s/it]\n",
      "100it [00:47,  2.09it/s]\n",
      "100it [01:16,  1.31it/s]\n",
      "100it [01:18,  1.27it/s]\n",
      "100it [01:36,  1.04it/s]\n",
      "100it [01:29,  1.12it/s]\n",
      "100it [01:56,  1.17s/it]\n",
      "100it [02:03,  1.23s/it]\n",
      "100it [01:10,  1.41it/s]\n",
      "100it [01:58,  1.19s/it]\n",
      "100it [01:24,  1.19it/s]\n",
      "100it [01:24,  1.19it/s]\n",
      "100it [00:54,  1.83it/s]\n",
      "100it [01:22,  1.21it/s]\n",
      "100it [01:13,  1.36it/s]\n",
      "100it [01:38,  1.01it/s]\n",
      "100it [00:44,  2.25it/s]\n",
      "100it [01:01,  1.64it/s]\n",
      "100it [01:33,  1.07it/s]\n",
      "100it [01:19,  1.25it/s]\n",
      "100it [01:11,  1.40it/s]\n",
      "100it [01:24,  1.18it/s]\n",
      "100it [00:45,  2.19it/s]\n",
      "100it [00:36,  2.71it/s]\n",
      "100it [00:41,  2.39it/s]\n",
      "100it [00:40,  2.45it/s]\n",
      "100it [00:38,  2.59it/s]\n",
      "100it [01:49,  1.09s/it]\n",
      "100it [01:11,  1.40it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [01:17,  1.29it/s]\n",
      "100it [01:09,  1.44it/s]\n",
      "100it [00:53,  1.88it/s]\n",
      "100it [00:55,  1.81it/s]\n",
      "100it [01:13,  1.36it/s]\n",
      "50it [00:33,  2.13it/s]PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n",
      "100it [01:07,  1.49it/s]\n",
      "100it [01:25,  1.17it/s]\n",
      "100it [00:37,  2.69it/s]\n",
      "100it [00:36,  2.73it/s]\n",
      "100it [00:30,  3.26it/s]\n",
      "100it [01:33,  1.07it/s]\n",
      "100it [00:27,  3.58it/s]\n",
      "100it [01:24,  1.18it/s]\n",
      "100it [01:47,  1.08s/it]\n",
      "100it [00:51,  1.92it/s]\n",
      "100it [02:22,  1.42s/it]\n",
      "100it [01:20,  1.24it/s]\n",
      "100it [01:14,  1.35it/s]\n",
      "100it [00:40,  2.49it/s]\n",
      "100it [00:46,  2.14it/s]\n",
      "100it [00:59,  1.69it/s]\n",
      "100it [00:42,  2.37it/s]\n",
      "100it [01:09,  1.43it/s]\n",
      "100it [00:47,  2.11it/s]\n",
      "100it [00:49,  2.02it/s]\n",
      "100it [01:01,  1.63it/s]\n",
      "100it [00:55,  1.81it/s]\n",
      "100it [00:44,  2.23it/s]\n",
      "100it [01:01,  1.61it/s]\n",
      "100it [00:47,  2.12it/s]\n",
      "100it [00:36,  2.74it/s]\n",
      "100it [01:01,  1.62it/s]\n",
      "100it [00:56,  1.76it/s]\n",
      "100it [00:39,  2.53it/s]\n",
      "100it [00:55,  1.79it/s]\n",
      "100it [00:41,  2.43it/s]\n",
      "100it [00:41,  2.39it/s]\n",
      "100it [00:51,  1.94it/s]\n",
      "100it [00:54,  1.84it/s]\n",
      "100it [00:54,  1.85it/s]\n",
      "100it [01:11,  1.40it/s]\n",
      "100it [00:40,  2.48it/s]\n",
      "100it [00:37,  2.69it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [00:50,  1.97it/s]\n",
      "100it [01:09,  1.44it/s]\n",
      "100it [01:11,  1.39it/s]\n",
      "100it [00:49,  2.03it/s]\n",
      "100it [00:48,  2.04it/s]\n",
      "100it [00:59,  1.69it/s]\n",
      "100it [01:18,  1.27it/s]\n",
      "100it [00:51,  1.94it/s]\n",
      "100it [01:24,  1.18it/s]\n",
      "100it [01:08,  1.46it/s]\n",
      "100it [00:57,  1.72it/s]\n",
      "100it [01:16,  1.30it/s]\n",
      "100it [00:39,  2.51it/s]\n",
      "100it [00:59,  1.68it/s]\n",
      "100it [01:12,  1.37it/s]\n",
      "100it [01:08,  1.46it/s]\n",
      "100it [01:03,  1.59it/s]\n",
      "100it [00:44,  2.25it/s]\n",
      "100it [00:55,  1.82it/s]\n",
      "100it [00:35,  2.85it/s]\n",
      "100it [00:36,  2.74it/s]\n",
      "100it [00:53,  1.88it/s]\n",
      "100it [01:07,  1.49it/s]\n",
      "100it [01:06,  1.51it/s]\n",
      "100it [01:01,  1.64it/s]\n",
      "100it [01:08,  1.45it/s]\n",
      "100it [00:35,  2.81it/s]\n",
      "100it [00:50,  1.99it/s]\n",
      "100it [00:57,  1.74it/s]\n",
      "100it [00:51,  1.95it/s]\n",
      "100it [00:42,  2.33it/s]\n",
      "100it [00:43,  2.30it/s]\n",
      "100it [00:48,  2.05it/s]\n",
      "100it [01:22,  1.22it/s]\n",
      "100it [00:51,  1.94it/s]\n",
      "100it [00:42,  2.36it/s]\n",
      "100it [00:35,  2.83it/s]\n",
      "100it [00:37,  2.68it/s]\n",
      "100it [01:08,  1.47it/s]\n",
      "100it [00:50,  1.96it/s]\n",
      "100it [01:11,  1.39it/s]\n",
      "100it [01:00,  1.65it/s]\n",
      "100it [02:15,  1.35s/it]\n",
      "100it [02:01,  1.22s/it]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [00:48,  2.08it/s]\n",
      "100it [01:04,  1.56it/s]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [00:56,  1.78it/s]\n",
      "100it [00:53,  1.85it/s]\n",
      "100it [01:35,  1.05it/s]\n",
      "100it [00:55,  1.80it/s]\n",
      "100it [01:12,  1.37it/s]\n",
      "100it [00:59,  1.69it/s]\n",
      "100it [01:21,  1.23it/s]\n",
      "100it [01:00,  1.66it/s]\n",
      "100it [00:37,  2.66it/s]\n",
      "100it [00:51,  1.94it/s]\n",
      "100it [00:49,  2.04it/s]\n",
      "100it [00:50,  1.97it/s]\n",
      "100it [00:48,  2.04it/s]\n",
      "100it [01:02,  1.61it/s]\n",
      "100it [00:41,  2.39it/s]\n",
      "100it [00:32,  3.03it/s]\n",
      "100it [01:01,  1.62it/s]\n",
      "100it [00:47,  2.11it/s]\n",
      "100it [01:20,  1.24it/s]\n",
      "100it [00:37,  2.63it/s]\n",
      "100it [01:55,  1.16s/it]\n",
      "100it [00:56,  1.78it/s]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [01:11,  1.39it/s]\n",
      "100it [01:13,  1.36it/s]\n",
      "100it [01:24,  1.18it/s]\n",
      "100it [01:00,  1.66it/s]\n",
      "100it [01:08,  1.45it/s]\n",
      "100it [01:15,  1.33it/s]\n",
      "100it [00:58,  1.70it/s]\n",
      "100it [01:01,  1.64it/s]\n",
      "100it [01:09,  1.43it/s]\n",
      "100it [01:05,  1.52it/s]\n",
      "100it [01:05,  1.52it/s]\n",
      "100it [00:51,  1.95it/s]\n",
      "100it [01:16,  1.31it/s]\n",
      "100it [01:23,  1.20it/s]\n",
      "100it [00:45,  2.18it/s]\n",
      "100it [00:52,  1.92it/s]\n",
      "100it [00:49,  2.04it/s]\n",
      "100it [00:55,  1.79it/s]\n",
      "100it [00:42,  2.36it/s]\n",
      "100it [00:35,  2.79it/s]\n",
      "100it [00:36,  2.71it/s]\n",
      "100it [00:50,  1.99it/s]\n",
      "100it [00:41,  2.42it/s]\n",
      "100it [00:31,  3.19it/s]\n",
      "100it [00:41,  2.40it/s]\n",
      "100it [00:45,  2.19it/s]\n",
      "100it [00:50,  1.97it/s]\n",
      "100it [01:00,  1.65it/s]\n",
      "100it [00:52,  1.89it/s]\n",
      "100it [00:52,  1.92it/s]\n",
      "100it [01:14,  1.34it/s]\n",
      "100it [01:12,  1.37it/s]\n",
      "100it [01:00,  1.64it/s]\n",
      "100it [00:46,  2.17it/s]\n",
      "100it [01:28,  1.14it/s]\n",
      "100it [01:32,  1.08it/s]\n",
      "100it [01:10,  1.41it/s]\n",
      "100it [00:47,  2.12it/s]\n",
      "100it [00:47,  2.11it/s]\n",
      "100it [01:06,  1.50it/s]\n",
      "100it [01:35,  1.05it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [01:01,  1.63it/s]\n",
      "100it [01:26,  1.15it/s]\n",
      "100it [00:49,  2.03it/s]\n",
      "100it [01:19,  1.26it/s]\n",
      "100it [01:21,  1.22it/s]\n",
      "100it [00:46,  2.14it/s]\n",
      "100it [01:48,  1.09s/it]\n",
      "100it [00:45,  2.18it/s]\n",
      "100it [01:18,  1.27it/s]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [01:29,  1.12it/s]\n",
      "100it [01:29,  1.11it/s]\n",
      "100it [00:40,  2.49it/s]\n",
      "100it [01:40,  1.00s/it]\n",
      "100it [01:13,  1.36it/s]\n",
      "100it [00:50,  1.97it/s]\n",
      "100it [00:46,  2.15it/s]\n",
      "100it [00:54,  1.82it/s]\n",
      "100it [01:17,  1.28it/s]\n",
      "100it [01:22,  1.21it/s]\n",
      "100it [01:36,  1.04it/s]\n",
      "100it [00:52,  1.89it/s]\n",
      "100it [00:31,  3.18it/s]\n",
      "100it [00:57,  1.75it/s]\n",
      "100it [00:55,  1.81it/s]\n",
      "100it [01:03,  1.57it/s]\n",
      "100it [01:07,  1.47it/s]\n",
      "100it [01:27,  1.15it/s]\n",
      "100it [00:55,  1.79it/s]\n",
      "100it [01:25,  1.16it/s]\n",
      "100it [01:02,  1.61it/s]\n",
      "100it [00:55,  1.81it/s]\n",
      "100it [00:56,  1.76it/s]\n",
      "100it [01:22,  1.21it/s]\n",
      "100it [00:51,  1.93it/s]\n",
      "100it [01:37,  1.03it/s]\n",
      "100it [00:49,  2.02it/s]\n",
      "100it [01:25,  1.17it/s]\n",
      "100it [01:14,  1.34it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [01:09,  1.44it/s]\n",
      "100it [01:45,  1.06s/it]\n",
      "100it [01:08,  1.46it/s]\n",
      "100it [01:00,  1.67it/s]\n",
      "100it [00:53,  1.87it/s]\n",
      "100it [01:33,  1.07it/s]\n",
      "100it [00:53,  1.87it/s]\n",
      "100it [00:50,  2.00it/s]\n",
      "100it [01:33,  1.07it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [01:38,  1.02it/s]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [00:48,  2.06it/s]\n",
      "100it [00:56,  1.77it/s]\n",
      "100it [01:19,  1.26it/s]\n",
      "100it [01:26,  1.16it/s]\n",
      "100it [01:28,  1.13it/s]\n",
      "100it [01:32,  1.08it/s]\n",
      "100it [01:18,  1.27it/s]\n",
      "100it [01:23,  1.19it/s]\n",
      "100it [00:58,  1.70it/s]\n",
      "100it [01:02,  1.61it/s]\n",
      "100it [01:40,  1.00s/it]\n",
      "100it [01:45,  1.06s/it]\n",
      "100it [01:41,  1.02s/it]\n",
      "100it [01:30,  1.11it/s]\n",
      "100it [01:04,  1.54it/s]\n",
      "100it [01:49,  1.09s/it]\n",
      "100it [01:43,  1.03s/it]\n",
      "100it [01:09,  1.43it/s]\n",
      "100it [01:30,  1.11it/s]\n",
      "100it [01:33,  1.07it/s]\n",
      "100it [01:00,  1.66it/s]\n",
      "100it [01:14,  1.33it/s]\n",
      "100it [01:11,  1.39it/s]\n",
      "100it [01:20,  1.25it/s]\n",
      "100it [00:48,  2.08it/s]\n",
      "100it [00:43,  2.31it/s]\n",
      "100it [00:37,  2.63it/s]\n",
      "100it [00:46,  2.16it/s]\n",
      "100it [00:52,  1.92it/s]\n",
      "100it [00:49,  2.02it/s]\n",
      "100it [01:06,  1.49it/s]\n",
      "100it [00:45,  2.22it/s]\n",
      "100it [00:28,  3.50it/s]\n",
      "100it [00:46,  2.13it/s]\n",
      "100it [00:35,  2.81it/s]\n",
      "100it [01:04,  1.54it/s]\n",
      "100it [00:56,  1.76it/s]\n",
      "100it [01:50,  1.11s/it]\n",
      "100it [01:12,  1.39it/s]\n",
      "100it [00:52,  1.92it/s]\n",
      "100it [01:10,  1.41it/s]\n",
      "100it [00:47,  2.10it/s]\n",
      "100it [00:46,  2.14it/s]\n",
      "100it [01:22,  1.21it/s]\n",
      "100it [01:01,  1.64it/s]\n",
      "100it [01:09,  1.44it/s]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [01:07,  1.48it/s]\n",
      "100it [01:16,  1.31it/s]\n",
      "100it [01:17,  1.28it/s]\n",
      "100it [01:12,  1.39it/s]\n",
      "100it [01:08,  1.45it/s]\n",
      "100it [00:55,  1.82it/s]\n",
      "100it [01:24,  1.18it/s]\n",
      "100it [00:55,  1.80it/s]\n",
      "100it [00:47,  2.12it/s]\n",
      "100it [00:50,  1.99it/s]\n",
      "100it [01:19,  1.26it/s]\n",
      "100it [01:42,  1.03s/it]\n",
      "100it [00:52,  1.91it/s]\n",
      "100it [01:04,  1.55it/s]\n",
      "100it [00:54,  1.84it/s]\n",
      "100it [00:38,  2.60it/s]\n",
      "100it [00:59,  1.68it/s]\n",
      "73it [00:49,  1.78it/s]https://www.sec.gov/Archives/edgar/data/0001385818/000119312515330782/d85936d10k.htm file will be skipped: (404) Not Found\n",
      "100it [01:01,  1.63it/s]\n",
      "100it [01:04,  1.55it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [00:39,  2.52it/s]\n",
      "100it [01:36,  1.04it/s]\n",
      "100it [00:34,  2.86it/s]\n",
      "100it [00:50,  1.98it/s]\n",
      "100it [00:46,  2.13it/s]\n",
      "100it [00:40,  2.45it/s]\n",
      "100it [00:59,  1.67it/s]\n",
      "100it [00:53,  1.86it/s]\n",
      "100it [00:45,  2.20it/s]\n",
      "100it [00:43,  2.30it/s]\n",
      "100it [00:56,  1.77it/s]\n",
      "100it [00:43,  2.30it/s]\n",
      "100it [00:37,  2.67it/s]\n",
      "100it [01:18,  1.28it/s]\n",
      "100it [01:04,  1.56it/s]\n",
      "100it [01:15,  1.32it/s]\n",
      "100it [00:46,  2.15it/s]\n",
      "100it [01:14,  1.35it/s]\n",
      "100it [00:55,  1.82it/s]\n",
      "100it [01:02,  1.59it/s]\n",
      "100it [01:15,  1.33it/s]\n",
      "100it [00:57,  1.75it/s]\n",
      "100it [01:27,  1.15it/s]\n",
      "100it [01:07,  1.49it/s]\n",
      "100it [01:06,  1.51it/s]\n",
      "100it [01:31,  1.09it/s]\n",
      "100it [00:41,  2.40it/s]\n",
      "100it [01:05,  1.52it/s]\n",
      "100it [00:53,  1.88it/s]\n",
      "100it [01:17,  1.29it/s]\n",
      "100it [00:56,  1.76it/s]\n",
      "100it [00:46,  2.14it/s]\n",
      "100it [01:05,  1.53it/s]\n",
      "100it [01:24,  1.18it/s]\n",
      "100it [01:07,  1.49it/s]\n",
      "100it [00:55,  1.81it/s]\n",
      "100it [00:47,  2.09it/s]\n",
      "100it [01:00,  1.65it/s]\n",
      "100it [00:41,  2.43it/s]\n",
      "100it [00:54,  1.83it/s]\n",
      "100it [00:46,  2.17it/s]\n",
      "100it [01:07,  1.48it/s]\n",
      "100it [01:03,  1.58it/s]\n",
      "100it [00:57,  1.74it/s]\n",
      "100it [00:39,  2.50it/s]\n",
      "100it [01:05,  1.53it/s]\n",
      "100it [01:23,  1.20it/s]\n",
      "100it [00:52,  1.89it/s]\n",
      "100it [00:41,  2.40it/s]\n",
      "100it [00:39,  2.52it/s]\n",
      "100it [01:05,  1.52it/s]\n",
      "100it [00:44,  2.26it/s]\n",
      "100it [00:53,  1.86it/s]\n",
      "100it [00:35,  2.84it/s]\n",
      "100it [01:21,  1.23it/s]\n",
      "100it [00:47,  2.10it/s]\n",
      "100it [01:10,  1.42it/s]\n",
      "100it [01:02,  1.59it/s]\n",
      "100it [00:56,  1.76it/s]\n",
      "100it [00:52,  1.91it/s]\n",
      "100it [00:39,  2.52it/s]\n",
      "100it [00:53,  1.86it/s]\n",
      "100it [00:45,  2.21it/s]\n",
      "100it [00:52,  1.91it/s]\n",
      "100it [00:54,  1.84it/s]\n",
      "100it [00:41,  2.38it/s]\n",
      "100it [00:55,  1.79it/s]\n",
      "100it [02:10,  1.30s/it]\n",
      "100it [01:19,  1.26it/s]\n",
      "100it [00:56,  1.76it/s]\n",
      "100it [01:07,  1.49it/s]\n",
      "10it [00:06,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing complete, please wait for the tasks to complete\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "_PHRASES = [\"keyword one\", \"keyword two\"]\n",
    "\n",
    "_FILING_TYPES = [\"10-K\", \"10-Q\"]\n",
    "\n",
    "_DATE_START = date.fromisoformat(\"2001-01-01\")\n",
    "_DATE_END = date.fromisoformat(\"2020-12-31\")\n",
    "\n",
    "# _INTERVAL = {\n",
    "#     \"years\": 0,\n",
    "#     \"months\": 1,\n",
    "#     \"weeks\": 0,\n",
    "#     \"days\": 0\n",
    "# }\n",
    "_INTERVAL = None # can be optional\n",
    "\n",
    "# _CIKS = [1961, \"0000003116\"] # accept a plain list of the CIKs\n",
    "# _CIKS = Path(\"ciks.txt\") # accept a file path\n",
    "# _CIKS = 1961 # accept a single CIK as an integer\n",
    "# _CIKS = \"0000003116\" # accept a single CIK as a string\n",
    "_CIKS = Path('sample_input_file.txt') # can be optional\n",
    "\n",
    "_CIKS_PER_QUERY = 5 # will be ignored if no CIKs is provided\n",
    "\n",
    "_BUFFER_CHUNK_SIZE = 100\n",
    "\n",
    "_EDGAR_RESULTS_PER_PAGE = 100\n",
    "\n",
    "_OUTPUT_NAME = \"sample_output_file\"\n",
    "\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "# User Agent Metadata\n",
    "_COMPANY_NAME = \"The University of Adelaide\"\n",
    "_COMPANY_DOMAIN_ADMIN_EMAIL = \"firstname.lastname@adelaide.edu.au\"\n",
    "\n",
    "_USER_AGENT = _COMPANY_NAME + \" \" + _COMPANY_DOMAIN_ADMIN_EMAIL\n",
    "\n",
    "await crawl(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _INTERVAL,\n",
    "    _CIKS,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT,\n",
    "    _USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda58edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cd2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
