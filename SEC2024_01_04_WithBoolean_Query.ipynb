{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration i \n",
    "\n",
    "----\n",
    "Update python to 3.10.13 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install --yes -c defaults -c conda-forge --update-all python=3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration ii\n",
    "\n",
    "---\n",
    "\n",
    "Uninstall package fitz, which may cause import conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iii\n",
    "\n",
    "---\n",
    "\n",
    "Go to your conda's base directory(e.g. `D:\\anaconda3`),under the path `D:\\anaconda3\\Lib\\site-packages`, **delete** all folders which name contain \"fitz\" if there any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iv\n",
    "\n",
    "---\n",
    "Use pip to install the necessary packages instead of using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp>=3.9.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: Faker>=18.9.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (22.0.0)\n",
      "Requirement already satisfied: pandas>=2.1.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.1.4)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2023.12.25)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (8.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: PyMuPDF>=1.23.8 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.23.8)\n",
      "Requirement already satisfied: ipykernel>=2.0.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (6.25.0)\n",
      "Collecting asyncio>=3.4.3 (from -r requirements.txt (line 10))\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from beautifulsoup4>=4.12.2->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from Faker>=18.9.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (1.26.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (2023.4)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from PyMuPDF>=1.23.8->-r requirements.txt (line 8)) (1.23.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (8.15.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.5.6)\n",
      "Requirement already satisfied: packaging in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (23.1)\n",
      "Requirement already satisfied: psutil in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (4.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=2.0.0->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from python-dateutil>=2.4->Faker>=18.9.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.9.0->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.2)\n",
      "Installing collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration v\n",
    "\n",
    "---\n",
    "press **Ctrl** + **SHFIT** + **P**, type **reload window** and select to reboot the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration vi\n",
    "\n",
    "---\n",
    "Following Section only need to be run at the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "import json, regex\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "async def __constant_update():\n",
    "    async with ClientSession(raise_for_status=True,headers=headers) as c:\n",
    "        async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "            _script = await res.text()\n",
    "\n",
    "        with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"_FORMS = \")\n",
    "            json.dump({\n",
    "                form.pop(\"form\"): form\n",
    "                for form in eval(regex.search(\n",
    "                    R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                    _script,\n",
    "                    regex.MULTILINE\n",
    "                )[1])\n",
    "            }, f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "            f.write(\"_LOCATIONS = \")\n",
    "            json.dump(dict(eval(regex.search(\n",
    "                R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "                _script,\n",
    "                regex.MULTILINE\n",
    "            )[1])), f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "await __constant_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date , timedelta\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession,ClientResponseError\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry,stop_after_attempt, wait_fixed,after_log,RetryError\n",
    "#import retry\n",
    "import time\n",
    "from tqdm import *\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "_DEFAULT_BYTE = b'{\"took\":530,\"timed_out\":false,\"_shards\":{\"total\":50,\"successful\":50,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":0,\"relation\":\"eq\"},\"max_score\":null,\"hits\":[]},\"aggregations\":{\"entity_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"sic_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"biz_states_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"form_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]}},\"query\":\"{\\\\\"query\\\\\":{\\\\\"bool\\\\\":{\\\\\"must\\\\\":[{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"data\\\\\"}},{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"breach\\\\\"}}],\\\\\"must_not\\\\\":[],\\\\\"should\\\\\":[],\\\\\"filter\\\\\":[{\\\\\"terms\\\\\":{\\\\\"ciks\\\\\":[\\\\\"0001199046\\\\\"]}},{\\\\\"range\\\\\":{\\\\\"file_date\\\\\":{\\\\\"gte\\\\\":\\\\\"2024-01-02\\\\\",\\\\\"lte\\\\\":\\\\\"2024-01-02\\\\\"}}}]}},\\\\\"from\\\\\":0,\\\\\"size\\\\\":100,\\\\\"aggregations\\\\\":{\\\\\"form_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"root_form\\\\\",\\\\\"size\\\\\":30}},\\\\\"entity_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"display_names.raw\\\\\",\\\\\"size\\\\\":30}},\\\\\"sic_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"sics\\\\\",\\\\\"size\\\\\":30}},\\\\\"biz_states_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"biz_states\\\\\",\\\\\"size\\\\\":30}}}}\"}'\n",
    "Boolean_KeyWord = lambda para: \" \".join(f\"\\\"{phrase}\\\"\" for phrase in para) if isinstance(para,list) else f\"\\\"{para}\\\"\"\n",
    "# @retry(stop=stop_after_attempt(10), wait=wait_fixed(2),reraise=False)\n",
    "# async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "#     q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "#     #forms = \" \".join(form for  form in forms)\n",
    "#     data = {'q':q,\n",
    "#             'startdt':start,\n",
    "#             'enddt':end,\n",
    "#             'ciks':cik,\n",
    "#             'dataRange':'custom',\n",
    "#             'category':'custom',\n",
    "#             'forms':forms}\n",
    "#     url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    \n",
    "#     async with semaphore, client.request(method='get', url=url, params=data) as res:\n",
    "#         await asyncio.sleep(1)\n",
    "#         try:\n",
    "#             if res.status == 200:\n",
    "#                 result = await res.read()\n",
    "#                 fetch_bar.update(1)\n",
    "#                 return result\n",
    "#         except  RetryError as e: #ClientResponseError or\n",
    "#             _LOGGER.warning(f\"{cik}-{start}-{end} query will be skipped: ({res.status}) {res.reason}\")\n",
    "#             fetch_bar.update(1)\n",
    "#             return _DEFAULT_BYTE\n",
    "    \n",
    "@retry(wait=wait_fixed(1))\n",
    "async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "    #q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "    #forms = \" \".join(form for  form in forms)\n",
    "    data = {'q':phrases,\n",
    "            'startdt':start,\n",
    "            'enddt':end,\n",
    "            'ciks':cik,\n",
    "            'dataRange':'custom',\n",
    "            'category':'custom',\n",
    "            'forms':forms}\n",
    "    url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    async with semaphore,client.request(method='get',url=url,params = data) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 200:\n",
    "            result = await res.read()\n",
    "            fetch_bar.update(1)\n",
    "            return result#await res.json()\n",
    "        raise ValueError(f\"Status Code = {res.status}\")\n",
    "    \n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(6), wait=wait_fixed(2),after=after_log(_LOGGER, logging.WARNING),reraise=False)#, \n",
    "async def _download(semaphore: asyncio.Semaphore, row_index, df,client,keywords,download_bar):\n",
    "    url = df.loc[row_index,\"url\"]\n",
    "    ext = df.loc[row_index,\"file_ext\"]\n",
    "    if ext not in [\"htm\",\"pdf\",\"txt\"]:\n",
    "        _LOGGER.warning(f\"Unkown extension in row [{row_index}]\")\n",
    "        tem = f\"paragrah{1}\"\n",
    "        df.loc[row_index,tem] = 'Unknow extension'\n",
    "        return download_bar.update(1)\n",
    "        \n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 404:\n",
    "            _LOGGER.warning(f\"Url: {url} is not exist: ({res.status}) {res.reason}. Skipping download.\")\n",
    "            return download_bar.update(1)\n",
    "        if res.status == 403 or res.status == 429:\n",
    "            _LOGGER.warning(f\"Reach request limitation: {url}: ({res.status}) {res.reason}.\\n Restart to crawl after 10 minutes. \")\n",
    "            await asyncio.sleep(630)\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                confirm = input(\"Please open 'https://www.sec.gov/edgar/search/' in your browser, press 'ok' to crawl continue if it available\")\n",
    "                if confirm.lower() == \"ok\":\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Manully confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        _LOGGER.warning(f\"Fail to Manully confirm: ({res.status}) {res.reason} \")\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - start_time\n",
    "                if elapsed_time >= 180 :\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Automatically confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        _LOGGER.warning(\"Fail to automate confirm. Will retry again after 10 mintues\")\n",
    "                        await asyncio.sleep(600)\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "        if res.ok:\n",
    "            try:\n",
    "                html = await res.read()\n",
    "                if ext == \"htm\":\n",
    "                    paragraphs = extract_html(html,keywords)\n",
    "                elif ext == \"pdf\":\n",
    "                    paragraphs = extract_pdf(html,keywords)\n",
    "                elif ext == \"txt\":\n",
    "                    paragraphs = extract_txt(html,keywords)\n",
    "                if not paragraphs:\n",
    "                    _LOGGER.warning(f\"No content extracted from {url}\")\n",
    "                    tem = f\"paragrah{1}\"\n",
    "                    df.loc[row_index, tem] = 'No content extracted'\n",
    "                    return download_bar.update(1)\n",
    "    \n",
    "                for num in range(len(paragraphs)):\n",
    "                    tem = f\"paragrah{num + 1}\"\n",
    "                    df.loc[row_index,tem] = str(paragraphs[num])\n",
    "                return download_bar.update(1)\n",
    "            except RetryError:\n",
    "                _LOGGER.warning(f\"Retring download url: {url} at the row: {row_index}\")\n",
    "                return download_bar.update(1)\n",
    "\n",
    "    _LOGGER.warning(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    tem = f\"paragrah{1}\"\n",
    "    df.loc[row_index, tem] = 'Download skipped'\n",
    "    #raise Exception(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    return download_bar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]): \n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    \n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), str(loc).split(\",\")[0], _LOCATIONS.get(code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"biz_states\"], #source[\"inc_states\"] if source[\"inc_states\"] else \n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        ) \n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], copy=False)#, dtype=str\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows.loc[0,\"cik\"]\n",
    "    info = pd.DataFrame({\n",
    "        \"entity_name\":rows['entity_name'],\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks, adsh.replace('-', ''), filename),\n",
    "        \"parser\": None#getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    },copy=False,dtype=str)#, dtype=object\n",
    "    \n",
    "    result = pd.merge(rows,info,how=\"left\",on=\"id\")\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "#len_check = lambda chunk,sub_delimiter: [chunk] if len(chunk) <=3000 else chunk.split(sub_delimiter)\n",
    "\n",
    "def extract_html(html, keywords):\n",
    "    matching_paragraphs = [regex.sub(r\"(\\s+)\",\" \",para).strip() \n",
    "                           for para in BeautifulSoup(html, \"lxml\",from_encoding='utf-8').get_text('\\n\\n').split('\\n\\n') \n",
    "                           if any(keyword in para.lower().replace(\"-\",\" \").replace(\"\\n\",\" \") \n",
    "                            for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "def extract_pdf(fetch,keywords):\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=\"pdf\")\n",
    "\n",
    "    total = [para.replace(\"\\n\",\" \") for page in range(pdf_document.page_count) \n",
    "            for lists in pdf_document[page].get_text(\"blocks\") \n",
    "            for  para in lists \n",
    "            if any(keyword in str(para).lower().replace(\"-\",\" \") for keyword in keywords) ]\n",
    "    return total\n",
    "\n",
    "def extract_txt(bytes,keywords):\n",
    "    txt = bytes.decode('utf-8').split('\\n\\n')\n",
    "    txt = [i.replace(\"\\n\",\" \")for i in txt]\n",
    "    matching_paragraphs = [paragraph for paragraph in txt if any(keyword in paragraph.lower().replace(\"-\",\" \") for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "    \n",
    "def chop_time_interval(start_date,end_date,frequency = \"2D\"):\n",
    "    interval = pd.interval_range(pd.Timestamp(start_date) ,pd.Timestamp(end_date),closed=\"both\",freq= frequency)\n",
    "    date_pattern = '%Y-%m-%d'\n",
    "    intervals = [((interval[i].left +timedelta(days=1)).strftime((date_pattern)), interval[i].right.strftime((date_pattern))) if i !=0 \n",
    "                 else ((interval[i].left.strftime((date_pattern)), interval[i].right.strftime((date_pattern))))\n",
    "                 for i in range(len(interval)) \n",
    "                 ]\n",
    "    return intervals\n",
    "\n",
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    if total_hits >= 100:\n",
    "        _LOGGER.warning(f\"Numbers opf Hits/entries >= 100. Try to reduce the 'interval of data range' or 'numerbes of per query' to avoid data missing.\")\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits\n",
    "def chop_ciks(\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int\n",
    ") -> Generator[Optional[List[str]], None, None]:\n",
    "    # defaults to None\n",
    "    _ciks: Optional[List[str]] = None\n",
    "    # if the provided parameter is a Path, read the CIKs from the file\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "\n",
    "    if _ciks:\n",
    "        for i in range(0, len(_ciks), ciks_per_query):\n",
    "            yield _ciks[i:i + ciks_per_query]\n",
    "    else:\n",
    "        yield None\n",
    "        \n",
    "def CIK(ciks):\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "    if _ciks == []:\n",
    "        _ciks = \"\"\n",
    "    return _ciks\n",
    "    \n",
    "async def main(_PHRASES,\n",
    "               _FILING_TYPES,\n",
    "               _DATE_START,\n",
    "               _DATE_END,\n",
    "               _CIKS_PER_QUERY, \n",
    "               _CIKS,\n",
    "               _buffer_chunk_size,\n",
    "               df,headers,\n",
    "               _OUTPUT_NAME,\n",
    "               _OUTPUT_FORMAT,\n",
    "               _DATA_RANGE_INTERVAL=\"2D\"):\n",
    "    \n",
    "    phrases = [Boolean_KeyWord(para) for para in _PHRASES ]\n",
    "    \n",
    "    if _OUTPUT_FORMAT not in [\"excel\",\"xlsx\",\"csv\"]:\n",
    "        raise TypeError(f\"{_OUTPUT_FORMAT} is not a valid extension. Options extension [excel,xlsx,csv].\")\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    if _FILING_TYPES == []:\n",
    "        _FILING_TYPES = [\"\"]\n",
    "    #-------------- Crawl --------\n",
    "\n",
    "    async with ClientSession(raise_for_status=False, headers=headers) as client :\n",
    "        #------Fetch--------\n",
    "        if CIK(_CIKS):\n",
    "            import math\n",
    "            total = math.ceil(len(CIK(_CIKS)) / _CIKS_PER_QUERY * len(phrases))\n",
    "            with tqdm(        \n",
    "                total=total) as fetch_bar:\n",
    "                _LOGGER.info(\"Starting fetch...\")\n",
    "                fetch_tasks = [\n",
    "                        asyncio.create_task(fetch(\n",
    "                            semaphore=semaphore,\n",
    "                            client=client,\n",
    "                            phrases=phrase,\n",
    "                            cik=ciks,\n",
    "                            start=_DATE_START,\n",
    "                            end=_DATE_END,\n",
    "                            forms=_FILING_TYPES,\n",
    "                            fetch_bar=fetch_bar\n",
    "                        ))\n",
    "                        for ciks in chop_ciks(_CIKS,_CIKS_PER_QUERY)\n",
    "                        for phrase in phrases\n",
    "                    ]\n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        else:\n",
    "            intervals = chop_time_interval(_DATE_START,_DATE_END,_DATA_RANGE_INTERVAL)\n",
    "            with tqdm(total=len(intervals) * len(phrases)) as fetch_bar:\n",
    "                fetch_tasks = [asyncio.create_task(fetch(\n",
    "                                    semaphore=semaphore,\n",
    "                                    client=client,\n",
    "                                    phrases=phrase,\n",
    "                                    cik=\"\",\n",
    "                                    start=start,\n",
    "                                    end=end,\n",
    "                                    forms=_FILING_TYPES,\n",
    "                                    fetch_bar=fetch_bar\n",
    "                                )) \n",
    "                               for start,end in intervals\n",
    "                               for phrase in phrases]\n",
    "                \n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        df = pd.concat([df] + \n",
    "                    [_parse_hit(hit) for data in fetched_data \n",
    "                     for hit in decode(data)],\n",
    "                    ignore_index=True)\n",
    "        \n",
    "        if df.shape[0] == 0:\n",
    "            _LOGGER.warning(\"Fetch completed with 0 result. Now existing.\")\n",
    "            return\n",
    "\n",
    "        \n",
    "        df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "        df = df.sample(frac=1.0)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        if _FILING_TYPES:\n",
    "            df = df[df['form_file'].apply(lambda x: any(keyword in x for keyword in _FILING_TYPES))]\n",
    "            if df.shape[0] == 0:\n",
    "                _LOGGER.error(f'Invalid FILING_TYPES: {_FILING_TYPES}, program exsit. ')\n",
    "        #-----Docs download-------\n",
    "        _LOGGER.info(f\"fetch completed and collected [{df.shape[0]}] of docs,starting download docs..\")\n",
    "        #keywords = [keyword for lists in _PHRASES for keyword in lists]\n",
    "        total = df.shape[0]\n",
    "        with tqdm(total=total) as download_bar:\n",
    "            for index in range(0,total,_buffer_chunk_size):\n",
    "                index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "                download_tasks = [\n",
    "                    asyncio.create_task(_download(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        df = df,\n",
    "                        row_index=row,\n",
    "                        download_bar=download_bar,\n",
    "                        keywords=[keyword for lists in _PHRASES for keyword in lists])\n",
    "                    )for row in index_range\n",
    "                    ]\n",
    "                downloaded = await asyncio.gather(*download_tasks)\n",
    "                \n",
    "\n",
    "\n",
    "    del df[\"parser\"] \n",
    "    del df['id']\n",
    "    if _OUTPUT_FORMAT in [\"excel\",\"xlsx\"]:\n",
    "        df.to_excel(f\"{_OUTPUT_NAME}.xlsx\")\n",
    "    elif _OUTPUT_FORMAT.lower() == \"csv\":      \n",
    "        df.to_csv(f\"{_OUTPUT_NAME}.csv\")\n",
    "    _LOGGER.info(f\"Data have been export at {os.getcwd()}\") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "- **df** (`pandas dataframe`) : Output data frame.\n",
    "\n",
    "- **headers** (`str`) : A string of user's email address for declaration.\n",
    "\n",
    "- **PHRASES** (`List[list,str]`): A list of lists that  keywords or phrases to search for. Every word in a nested list represents the 'and' boolean (appear at the same time) of the nested list.\n",
    "\n",
    "    \n",
    "    E.g. \n",
    "    1. [ [\"data breach\", \"cyber security\"], [\"fiscal deficit\"] ] = (\"data breach\" **and** \"cyber security\") **or** \"fiscal deficit\"\n",
    "    \n",
    "    2. [ [\"data breach\"], [\"cyber security\"], [\"fiscal deficit\"] ] = \"data breach\" **or** \"cyber security\" **or** \"fiscal deficit\" \n",
    "    \n",
    "     **as the same as** [\"data breach\", \"cyber security\",\"fiscal deficit\"] = \"data breach\" **or** \"cyber security\" **or** \"fiscal deficit\", while this format may casue unexpected result.\n",
    "    \n",
    "    ---\n",
    "\n",
    "\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **DATA_RANGE_INTERVAL** (`str`):\n",
    "    -  A string to set up a data range interval for per query. Enable when do not providing an specific CIKs input, i.e., \"1D\",\"3D\" and \"**X**D\".\n",
    "    -  Reducing the interval will result in more queires have to be made. Recommanded set as not more than \"5D\" since may return more than 100 enties per query and casue data missing.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): \n",
    "    - A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query. Can be set either empty or specify(e.g, path/list/integer). \n",
    "    - If it set as empty, query will base on ***date*** instead of CIKs.\n",
    "\n",
    "- **CIKS_PER_QUERY** (`int`): \n",
    "    - Controls the number of CIKs included in one query. \n",
    "    - Recommended value is 1. By the most recent testing(4 Jan 2024), I found out that server will only return information of **the last CIK** if we query multiple CIKs each time.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE** (`int`): The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT** (`enumerate[\"excel\",\"xlsx\",\"csv\"]`): The file format of the output file.\n",
    "\n",
    "---\n",
    "\n",
    "A test refernce:\n",
    "\n",
    "https://efts.sec.gov/LATEST/search-index?q=%22data+breach%22+%22cyber+security%22&q=%22fiscal%20+%20deficit%22&startdt=2020-01-10&enddt=2024-01-03&ciks=&dataRange=custom&category=custom&forms=10-K\n",
    "\n",
    "Versus input:\n",
    "- keywords : (\"data breach\" and \"cyber security\") or \"fiscal deficit\"\n",
    "- DATE_START : \"2020-01-01\" and DATE_END : \"2024-01-03\"\n",
    "- FILING_TYPES : \"10-K\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2423 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2423/2423 [18:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch completed and collected [917] of docs,starting download docs..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 917/917 [08:03<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data have been export at /mnt/d/summer_research\n",
      "--------------------\n",
      "All tasks completed! Time Cost:26.1 minutes \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "T0 = time.time()\n",
    "df = pd.DataFrame()\n",
    "headers = {\"User-Agent\":\"a1835067@student.adelaide.edu.au\"}\n",
    "#_PHRASES = [[\"data breach\", \"cyber security\"],[\"fiscal deficit\"]]\n",
    "_PHRASES = [[\"data breach\", \"cyber security\"]] #需要调整逻辑，anr可以在单一query，但or只能另开query\n",
    "_DATE_START = \"2001-01-01\"\n",
    "_DATE_END = \"2024-01-03\"\n",
    "_FILING_TYPES = []\n",
    "### Query based on date\n",
    "_DATA_RANGE_INTERVAL = \"5D\"\n",
    "#_CIKS =  [] \n",
    "### Query Base on CIks\n",
    "_CIKS = Path(\"sample_input_file.txt\") ####Please mannually select N here for Testing for the first N CIKs###\n",
    "_CIKS_PER_QUERY = 1\n",
    "####################\n",
    "_BUFFER_CHUNK_SIZE = 10\n",
    "_OUTPUT_NAME = f\"{_DATE_START}_to_{_DATE_END }_withCIKs_{_CIKS_PER_QUERY}PerQuery_Boolean\"\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "\n",
    "df = await main(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _CIKS,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    df,\n",
    "    headers,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT,\n",
    "    _DATA_RANGE_INTERVAL\n",
    ")\n",
    " \n",
    "END = time.time()\n",
    "print(\"--\"*10,\n",
    "      f\"All tasks completed! Time Cost:{round((END-T0)/60,1)} minutes \",sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['10', '20']\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_FILING_TYPES = ['10','20']\n",
    "f'{_FILING_TYPES}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
