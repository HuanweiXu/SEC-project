{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration i \n",
    "\n",
    "----\n",
    "Update python to 3.10.13 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install --yes -c defaults -c conda-forge --update-all python=3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration ii\n",
    "\n",
    "---\n",
    "\n",
    "Uninstall package fitz, which may cause import conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iii\n",
    "\n",
    "---\n",
    "\n",
    "Go to your conda's base directory(e.g. `D:\\anaconda3`),under the path `D:\\anaconda3\\Lib\\site-packages`, **delete** all folders which name contain \"fitz\" if there any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iv\n",
    "\n",
    "---\n",
    "Use pip to install the necessary packages instead of using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp>=3.9.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: Faker>=18.9.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (22.0.0)\n",
      "Requirement already satisfied: pandas>=2.1.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.1.4)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2023.12.25)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (8.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
      "Requirement already satisfied: PyMuPDF>=1.23.8 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.23.8)\n",
      "Requirement already satisfied: ipykernel>=2.0.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (6.25.0)\n",
      "Collecting asyncio>=3.4.3 (from -r requirements.txt (line 10))\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from aiohttp>=3.9.0->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from beautifulsoup4>=4.12.2->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from Faker>=18.9.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (1.26.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pandas>=2.1.4->-r requirements.txt (line 4)) (2023.4)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from PyMuPDF>=1.23.8->-r requirements.txt (line 8)) (1.23.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (8.15.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.5.6)\n",
      "Requirement already satisfied: packaging in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (23.1)\n",
      "Requirement already satisfied: psutil in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.7.1)\n",
      "Requirement already satisfied: backcall in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (4.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=2.0.0->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from python-dateutil>=2.4->Faker>=18.9.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.9.0->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/gray/miniconda3/envs/test/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9)) (0.2.2)\n",
      "Installing collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration v\n",
    "\n",
    "---\n",
    "press **Ctrl** + **SHFIT** + **P**, type **reload window** and select to reboot the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration vi\n",
    "\n",
    "---\n",
    "Following Section only need to be run at the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "import json, regex\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "async def __constant_update():\n",
    "    async with ClientSession(raise_for_status=True,headers=headers) as c:\n",
    "        async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "            _script = await res.text()\n",
    "\n",
    "        with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"_FORMS = \")\n",
    "            json.dump({\n",
    "                form.pop(\"form\"): form\n",
    "                for form in eval(regex.search(\n",
    "                    R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                    _script,\n",
    "                    regex.MULTILINE\n",
    "                )[1])\n",
    "            }, f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "            f.write(\"_LOCATIONS = \")\n",
    "            json.dump(dict(eval(regex.search(\n",
    "                R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "                _script,\n",
    "                regex.MULTILINE\n",
    "            )[1])), f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "await __constant_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date , timedelta\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession,ClientResponseError\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry,stop_after_attempt, wait_fixed,after_log,RetryError\n",
    "#import retry\n",
    "import time\n",
    "from tqdm import *\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "_DEFAULT_BYTE = b'{\"took\":530,\"timed_out\":false,\"_shards\":{\"total\":50,\"successful\":50,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":0,\"relation\":\"eq\"},\"max_score\":null,\"hits\":[]},\"aggregations\":{\"entity_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"sic_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"biz_states_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"form_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]}},\"query\":\"{\\\\\"query\\\\\":{\\\\\"bool\\\\\":{\\\\\"must\\\\\":[{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"data\\\\\"}},{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"breach\\\\\"}}],\\\\\"must_not\\\\\":[],\\\\\"should\\\\\":[],\\\\\"filter\\\\\":[{\\\\\"terms\\\\\":{\\\\\"ciks\\\\\":[\\\\\"0001199046\\\\\"]}},{\\\\\"range\\\\\":{\\\\\"file_date\\\\\":{\\\\\"gte\\\\\":\\\\\"2024-01-02\\\\\",\\\\\"lte\\\\\":\\\\\"2024-01-02\\\\\"}}}]}},\\\\\"from\\\\\":0,\\\\\"size\\\\\":100,\\\\\"aggregations\\\\\":{\\\\\"form_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"root_form\\\\\",\\\\\"size\\\\\":30}},\\\\\"entity_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"display_names.raw\\\\\",\\\\\"size\\\\\":30}},\\\\\"sic_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"sics\\\\\",\\\\\"size\\\\\":30}},\\\\\"biz_states_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"biz_states\\\\\",\\\\\"size\\\\\":30}}}}\"}'\n",
    "Boolean_KeyWord = lambda para: \" \".join(f\"\\\"{phrase}\\\"\" for phrase in para) if isinstance(para,list) else f\"\\\"{para}\\\"\"\n",
    "# @retry(stop=stop_after_attempt(10), wait=wait_fixed(2),reraise=False)\n",
    "# async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "#     q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "#     #forms = \" \".join(form for  form in forms)\n",
    "#     data = {'q':q,\n",
    "#             'startdt':start,\n",
    "#             'enddt':end,\n",
    "#             'ciks':cik,\n",
    "#             'dataRange':'custom',\n",
    "#             'category':'custom',\n",
    "#             'forms':forms}\n",
    "#     url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    \n",
    "#     async with semaphore, client.request(method='get', url=url, params=data) as res:\n",
    "#         await asyncio.sleep(1)\n",
    "#         try:\n",
    "#             if res.status == 200:\n",
    "#                 result = await res.read()\n",
    "#                 fetch_bar.update(1)\n",
    "#                 return result\n",
    "#         except  RetryError as e: #ClientResponseError or\n",
    "#             _LOGGER.warning(f\"{cik}-{start}-{end} query will be skipped: ({res.status}) {res.reason}\")\n",
    "#             fetch_bar.update(1)\n",
    "#             return _DEFAULT_BYTE\n",
    "    \n",
    "@retry(wait=wait_fixed(1))\n",
    "async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "             \n",
    "    #q = \" \".join(f\"\\\"{phrase}\\\"\" for phrase in phrases)\n",
    "    #forms = \" \".join(form for  form in forms)\n",
    "    data = {'q':phrases,\n",
    "            'startdt':start,\n",
    "            'enddt':end,\n",
    "            'ciks':cik,\n",
    "            'dataRange':'custom',\n",
    "            'category':'custom',\n",
    "            'forms':forms}\n",
    "    url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    async with semaphore,client.request(method='get',url=url,params = data) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 200:\n",
    "            result = await res.read()\n",
    "            fetch_bar.update(1)\n",
    "            return result#await res.json()\n",
    "        raise ValueError(f\"Status Code = {res.status}\")\n",
    "    \n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(6), wait=wait_fixed(2),after=after_log(_LOGGER, logging.WARNING),reraise=False)#, \n",
    "async def _download(semaphore: asyncio.Semaphore, row_index, df,client,keywords,download_bar):\n",
    "    url = df.loc[row_index,\"url\"]\n",
    "    ext = df.loc[row_index,\"file_ext\"]\n",
    "    if ext not in [\"htm\",\"pdf\",\"txt\"]:\n",
    "        _LOGGER.warning(f\"Unkown extension in row [{row_index}]\")\n",
    "        tem = f\"paragrah{1}\"\n",
    "        df.loc[row_index,tem] = 'Unknow extension'\n",
    "        return download_bar.update(1)\n",
    "        \n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 404:\n",
    "            _LOGGER.warning(f\"Url: {url} is not exist: ({res.status}) {res.reason}. Skipping download.\")\n",
    "            return download_bar.update(1)\n",
    "        if res.status == 403 or res.status == 429:\n",
    "            _LOGGER.warning(f\"Reach request limitation: {url}: ({res.status}) {res.reason}.\\n Restart to crawl after 10 minutes. \")\n",
    "            await asyncio.sleep(630)\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                confirm = input(\"Please open 'https://www.sec.gov/edgar/search/' in your browser, press 'ok' to crawl continue if it available\")\n",
    "                if confirm.lower() == \"ok\":\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Manully confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        _LOGGER.warning(f\"Fail to Manully confirm: ({res.status}) {res.reason} \")\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - start_time\n",
    "                if elapsed_time >= 180 :\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Automatically confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        _LOGGER.warning(\"Fail to automate confirm. Will retry again after 10 mintues\")\n",
    "                        await asyncio.sleep(600)\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "        if res.ok:\n",
    "            try:\n",
    "                html = await res.read()\n",
    "                if ext == \"htm\":\n",
    "                    paragraphs = extract_html(html,keywords)\n",
    "                elif ext == \"pdf\":\n",
    "                    paragraphs = extract_pdf(html,keywords)\n",
    "                elif ext == \"txt\":\n",
    "                    paragraphs = extract_txt(html,keywords)\n",
    "                if not paragraphs:\n",
    "                    _LOGGER.warning(f\"No content extracted from {url}\")\n",
    "                    tem = f\"paragrah{1}\"\n",
    "                    df.loc[row_index, tem] = 'No content extracted'\n",
    "                    return download_bar.update(1)\n",
    "    \n",
    "                for num in range(len(paragraphs)):\n",
    "                    tem = f\"paragrah{num + 1}\"\n",
    "                    df.loc[row_index,tem] = str(paragraphs[num])\n",
    "                return download_bar.update(1)\n",
    "            except RetryError:\n",
    "                _LOGGER.warning(f\"Retring download url: {url} at the row: {row_index}\")\n",
    "                return download_bar.update(1)\n",
    "\n",
    "    _LOGGER.warning(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    tem = f\"paragrah{1}\"\n",
    "    df.loc[row_index, tem] = 'Download skipped'\n",
    "    #raise Exception(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    return download_bar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]): \n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    \n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), str(loc).split(\",\")[0], _LOCATIONS.get(code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"biz_states\"], #source[\"inc_states\"] if source[\"inc_states\"] else \n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        ) \n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], copy=False)#, dtype=str\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows.loc[0,\"cik\"]\n",
    "    info = pd.DataFrame({\n",
    "        \"entity_name\":rows['entity_name'],\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks, adsh.replace('-', ''), filename),\n",
    "        \"parser\": None#getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    },copy=False,dtype=str)#, dtype=object\n",
    "    \n",
    "    result = pd.merge(rows,info,how=\"left\",on=\"id\")\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "#len_check = lambda chunk,sub_delimiter: [chunk] if len(chunk) <=3000 else chunk.split(sub_delimiter)\n",
    "\n",
    "def extract_html(html, keywords):\n",
    "    matching_paragraphs = [regex.sub(r\"(\\s+)\",\" \",para).strip() \n",
    "                           for para in BeautifulSoup(html, \"lxml\",from_encoding='utf-8').get_text('\\n\\n').split('\\n\\n') \n",
    "                           if any(keyword in para.lower().replace(\"-\",\" \").replace(\"\\n\",\" \") \n",
    "                            for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "def extract_pdf(fetch,keywords):\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=\"pdf\")\n",
    "\n",
    "    total = [para.replace(\"\\n\",\" \") for page in range(pdf_document.page_count) \n",
    "            for lists in pdf_document[page].get_text(\"blocks\") \n",
    "            for  para in lists \n",
    "            if any(keyword in str(para).lower().replace(\"-\",\" \") for keyword in keywords) ]\n",
    "    return total\n",
    "\n",
    "def extract_txt(bytes,keywords):\n",
    "    txt = bytes.decode('utf-8').split('\\n\\n')\n",
    "    txt = [i.replace(\"\\n\",\" \")for i in txt]\n",
    "    matching_paragraphs = [paragraph for paragraph in txt if any(keyword in paragraph.lower().replace(\"-\",\" \") for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "    \n",
    "def chop_time_interval(start_date,end_date,frequency = \"2D\"):\n",
    "    interval = pd.interval_range(pd.Timestamp(start_date) ,pd.Timestamp(end_date),closed=\"both\",freq= frequency)\n",
    "    date_pattern = '%Y-%m-%d'\n",
    "    intervals = [((interval[i].left +timedelta(days=1)).strftime((date_pattern)), interval[i].right.strftime((date_pattern))) if i !=0 \n",
    "                 else ((interval[i].left.strftime((date_pattern)), interval[i].right.strftime((date_pattern))))\n",
    "                 for i in range(len(interval)) \n",
    "                 ]\n",
    "    return intervals\n",
    "\n",
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    if total_hits >= 100:\n",
    "        _LOGGER.warning(f\"Numbers opf Hits/entries >= 100. Try to reduce the interval of data range to avoid data missing.\")\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    #print(hits)\n",
    "    return hits\n",
    "def chop_ciks(\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int\n",
    ") -> Generator[Optional[List[str]], None, None]:\n",
    "    # defaults to None\n",
    "    _ciks: Optional[List[str]] = None\n",
    "    # if the provided parameter is a Path, read the CIKs from the file\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "\n",
    "    if _ciks:\n",
    "        for i in range(0, len(_ciks), ciks_per_query):\n",
    "            yield _ciks[i:i + ciks_per_query]\n",
    "    else:\n",
    "        yield None\n",
    "        \n",
    "def CIK(ciks):\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "    if _ciks == []:\n",
    "        _ciks = \"\"\n",
    "    return _ciks\n",
    "    \n",
    "async def main(_PHRASES,\n",
    "               _FILING_TYPES,\n",
    "               _DATE_START,\n",
    "               _DATE_END,\n",
    "               _CIKS_PER_QUERY, \n",
    "               _CIKS,\n",
    "               _buffer_chunk_size,\n",
    "               df,headers,\n",
    "               _OUTPUT_NAME,\n",
    "               _OUTPUT_FORMAT,\n",
    "               _DATA_RANGE_INTERVAL=\"2D\"):\n",
    "    \n",
    "    phrases = [Boolean_KeyWord(para) for para in _PHRASES ]\n",
    "    if _OUTPUT_FORMAT not in [\"excel\",\"xlsx\",\"csv\"]:\n",
    "        raise TypeError(f\"{_OUTPUT_FORMAT} is not a valid extension. Options extension [excel,xlsx,csv].\")\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    if _FILING_TYPES == []:\n",
    "        _FILING_TYPES = [\"\"]\n",
    "    #-------------- Crawl --------\n",
    "\n",
    "    async with ClientSession(raise_for_status=False, headers=headers) as client :\n",
    "        #------Fetch--------\n",
    "        if CIK(_CIKS):\n",
    "            import math\n",
    "            total = math.ceil(len(CIK(_CIKS)) / _CIKS_PER_QUERY )\n",
    "            with tqdm(        \n",
    "                total=total) as fetch_bar:\n",
    "                print(\"Starting fetch...\")\n",
    "                fetch_tasks = [\n",
    "                        asyncio.create_task(fetch(\n",
    "                            semaphore=semaphore,\n",
    "                            client=client,\n",
    "                            phrases=phrases,\n",
    "                            cik=ciks,\n",
    "                            start=_DATE_START,\n",
    "                            end=_DATE_END,\n",
    "                            forms=_FILING_TYPES,\n",
    "                            fetch_bar=fetch_bar\n",
    "                        ))\n",
    "                        for ciks in chop_ciks(_CIKS,_CIKS_PER_QUERY)\n",
    "                    ]\n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        else:\n",
    "            intervals = chop_time_interval(_DATE_START,_DATE_END,_DATA_RANGE_INTERVAL)\n",
    "            with tqdm(total=len(intervals)) as fetch_bar:\n",
    "                fetch_tasks = [asyncio.create_task(fetch(\n",
    "                                    semaphore=semaphore,\n",
    "                                    client=client,\n",
    "                                    phrases=phrases,\n",
    "                                    cik=\"\",\n",
    "                                    start=start,\n",
    "                                    end=end,\n",
    "                                    forms=_FILING_TYPES,\n",
    "                                    fetch_bar=fetch_bar\n",
    "                                )) \n",
    "                               for start,end in intervals]\n",
    "                \n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        df = pd.concat([df] + \n",
    "                    [_parse_hit(hit) for data in fetched_data \n",
    "                     for hit in decode(data)],\n",
    "                    ignore_index=True)\n",
    "        df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "        df = df.sample(frac=1.0)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        #-----Docs download-------\n",
    "        print(f\"fetch completed and collected [{df.shape[0]}] of docs,starting download docs..\")\n",
    "        # total = df.shape[0]\n",
    "        # if total == 0:\n",
    "        #     print(\"Fetch completed with 0 result. Now existing.\")\n",
    "        #     return\n",
    "        # with tqdm(total=total) as download_bar:\n",
    "        #     for index in range(0,total,_buffer_chunk_size):\n",
    "        #         index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "        #         download_tasks = [\n",
    "        #             asyncio.create_task(_download(\n",
    "        #                 semaphore=semaphore,\n",
    "        #                 client=client,\n",
    "        #                 df = df,\n",
    "        #                 row_index=row,\n",
    "        #                 download_bar=download_bar,\n",
    "        #                 keywords=_PHRASES)\n",
    "        #             )for row in index_range\n",
    "        #             ]\n",
    "        #         downloaded = await asyncio.gather(*download_tasks)\n",
    "                \n",
    "\n",
    "\n",
    "    del df[\"parser\"] \n",
    "    del df['id']\n",
    "    if _OUTPUT_FORMAT in [\"excel\",\"xlsx\"]:\n",
    "        df.to_excel(f\"{_OUTPUT_NAME}.xlsx\")\n",
    "    elif _OUTPUT_FORMAT.lower() == \"csv\":      \n",
    "        df.to_csv(f\"{_OUTPUT_NAME}.csv\")\n",
    "    print(f\"Data have been export at {os.getcwd()}\") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "- **df** (`pandas dataframe`) : Output data frame.\n",
    "\n",
    "- **headers** (`str`) : A string of user's email address for declaration.\n",
    "\n",
    "- **PHRASES** (`List[list,str]`): A list of lists that  keywords or phrases to search for. Every word in a nested list represents the 'and' boolean (appear at the same time) of the nested list.\n",
    "\n",
    "    ---\n",
    "    E.g. \n",
    "    1. [ [\"data breach\", \"cyber security\"], [\"fiscal deficit\"] ] = (\"data breach\" **and** \"cyber security\") **or** \"fiscal deficit\"\n",
    "    \n",
    "    2. [ [\"data breach\"], [\"cyber security\"], [\"fiscal deficit\"] ] = \"data breach\" **or** \"cyber security\" **or** \"fiscal deficit\" \n",
    "    \n",
    "     **as the same as** [\"data breach\", \"cyber security\",\"fiscal deficit\"] = \"data breach\" **or** \"cyber security\" **or** \"fiscal deficit\"\n",
    "    \n",
    "    ---\n",
    "\n",
    "\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **DATA_RANGE_INTERVAL** (`str`): A string to set up a data range interval for per query. Enable when do not providing an specific CIKs input, i.e., \"1D\",\"3D\" and \"**X**D\". Reducing the interval will result in more queires have to be made. Recommanded set as not more than \"5D\" since may return more than 100 enties per query and casue data missing.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query.\n",
    "\n",
    "- **CIKS_PER_QUERY** (`int`): Controls the number of CIKs included in one query. Recommended value is 5, but can be adjusted in case the number of results returned exceed the maximum capacity (10000) in one query.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE**: The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT** (`enumerate[\"excel\",\"xlsx\",\"csv\"]`): The file format of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/243 [00:04<03:42,  1.07it/s]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 67\u001b[0m, in \u001b[0;36mfetch\u001b[0;34m(fetch_bar, semaphore, client, phrases, cik, end, forms, start, range, category, entity)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore,client\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m'\u001b[39m,url\u001b[38;5;241m=\u001b[39murl,params \u001b[38;5;241m=\u001b[39m data) \u001b[38;5;28;01mas\u001b[39;00m res:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/summer/lib/python3.10/asyncio/tasks.py:605\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# _OUTPUT_FORMAT = \"csv\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m _OUTPUT_FORMAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main(\n\u001b[1;32m     23\u001b[0m     _PHRASES,\n\u001b[1;32m     24\u001b[0m     _FILING_TYPES,\n\u001b[1;32m     25\u001b[0m     _DATE_START,\n\u001b[1;32m     26\u001b[0m     _DATE_END,\n\u001b[1;32m     27\u001b[0m     _CIKS_PER_QUERY,\n\u001b[1;32m     28\u001b[0m     _CIKS,\n\u001b[1;32m     29\u001b[0m     _BUFFER_CHUNK_SIZE,\n\u001b[1;32m     30\u001b[0m     df,\n\u001b[1;32m     31\u001b[0m     headers,\n\u001b[1;32m     32\u001b[0m     _OUTPUT_NAME,\n\u001b[1;32m     33\u001b[0m     _OUTPUT_FORMAT,\n\u001b[1;32m     34\u001b[0m     _DATA_RANGE_INTERVAL\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m END \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     39\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tasks completed! Time Cost:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m((END\u001b[38;5;241m-\u001b[39mT0)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes \u001b[39m\u001b[38;5;124m\"\u001b[39m,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 356\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(_PHRASES, _FILING_TYPES, _DATE_START, _DATE_END, _CIKS_PER_QUERY, _CIKS, _buffer_chunk_size, df, headers, _OUTPUT_NAME, _OUTPUT_FORMAT, _DATA_RANGE_INTERVAL)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fetch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m         fetch_tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    343\u001b[0m                 asyncio\u001b[38;5;241m.\u001b[39mcreate_task(fetch(\n\u001b[1;32m    344\u001b[0m                     semaphore\u001b[38;5;241m=\u001b[39msemaphore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m ciks \u001b[38;5;129;01min\u001b[39;00m chop_ciks(_CIKS,_CIKS_PER_QUERY)\n\u001b[1;32m    355\u001b[0m             ]\n\u001b[0;32m--> 356\u001b[0m         fetched_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mfetch_tasks)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     intervals \u001b[38;5;241m=\u001b[39m chop_time_interval(_DATE_START,_DATE_END,_DATA_RANGE_INTERVAL)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "T0 = time.time()\n",
    "df = pd.DataFrame()\n",
    "headers = {\"User-Agent\":\"a1835067@student.adelaide.edu.au\"}\n",
    "_PHRASES = [[\"data breach\", \"cyber security\"],[\"fiscal deficit\"]]\n",
    "_DATA_RANGE_INTERVAL = \"5D\" # \n",
    "#_FILING_TYPES = [\"10-K\"]#,\n",
    "_FILING_TYPES = [\"\"]\n",
    "_DATE_START = \"2001-01-01\"\n",
    "_DATE_END = \"2024-01-03\"\n",
    "_CIKS_PER_QUERY = 10\n",
    "_BUFFER_CHUNK_SIZE = 10\n",
    "#_CIKS =  [] \n",
    "_CIKS = Path(\"sample_input_file.txt\") ####Please mannually select N here for Testing for the first N CIKs###\n",
    "_OUTPUT_NAME = \"20240103_withKIs_full_2D_Boolean\"\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "\n",
    "df = await main(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _CIKS,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    df,\n",
    "    headers,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT,\n",
    "    _DATA_RANGE_INTERVAL\n",
    ")\n",
    " \n",
    "END = time.time()\n",
    "print(\"--\"*10,\n",
    "      f\"All tasks completed! Time Cost:{round((END-T0)/60,1)} minutes \",sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name_x</th>\n",
       "      <th>cik</th>\n",
       "      <th>located</th>\n",
       "      <th>incorporated</th>\n",
       "      <th>file_num</th>\n",
       "      <th>film_num</th>\n",
       "      <th>entity_name_y</th>\n",
       "      <th>form_file</th>\n",
       "      <th>file_date</th>\n",
       "      <th>period_ending</th>\n",
       "      <th>file_ext</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grupo Aval Acciones Y Valores S.A.  (AVAL)</td>\n",
       "      <td>0001504764</td>\n",
       "      <td>Bogota D.c.</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>001-36631</td>\n",
       "      <td>221400290</td>\n",
       "      <td>Grupo Aval Acciones Y Valores S.A.  (AVAL)</td>\n",
       "      <td>6-K (Current report)</td>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REPUBLIC OF CHILE</td>\n",
       "      <td>0000019957</td>\n",
       "      <td>Santiago Chile</td>\n",
       "      <td>Chile</td>\n",
       "      <td>001-02574</td>\n",
       "      <td>17906270</td>\n",
       "      <td>REPUBLIC OF CHILE</td>\n",
       "      <td>18-K (Annual report) EX-99.D</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Millburn Multi-Markets Fund L.P.</td>\n",
       "      <td>0001468910</td>\n",
       "      <td>Greenwich</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>000-54028</td>\n",
       "      <td>181178816</td>\n",
       "      <td>Millburn Multi-Markets Fund L.P.</td>\n",
       "      <td>10-Q (Quarterly report)</td>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ternium S.A.  (TX)</td>\n",
       "      <td>0001342874</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>001-32734</td>\n",
       "      <td>23781454</td>\n",
       "      <td>Ternium S.A.  (TX)</td>\n",
       "      <td>20-F (Annual report - foreign issuer)</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WisdomTree Trust</td>\n",
       "      <td>0001350487</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>333-132380</td>\n",
       "      <td>181172160</td>\n",
       "      <td>WisdomTree Trust</td>\n",
       "      <td>485APOS (Prospectus materials)</td>\n",
       "      <td>2018-11-09</td>\n",
       "      <td>None</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Direxion Shares ETF Trust</td>\n",
       "      <td>0001424958</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>333-150525</td>\n",
       "      <td>221112309</td>\n",
       "      <td>Direxion Shares ETF Trust</td>\n",
       "      <td>497 (Prospectus)</td>\n",
       "      <td>2022-07-28</td>\n",
       "      <td>None</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>EDENOR  (EDN)</td>\n",
       "      <td>0001395213</td>\n",
       "      <td>City of Buenos Aires</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>001-33422</td>\n",
       "      <td>23829818</td>\n",
       "      <td>EDENOR  (EDN)</td>\n",
       "      <td>20-F (Annual report - foreign issuer)</td>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>DIREXION FUNDS</td>\n",
       "      <td>0001040587</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>333-28697</td>\n",
       "      <td>18723487</td>\n",
       "      <td>DIREXION FUNDS</td>\n",
       "      <td>485APOS (Prospectus materials)</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>None</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>T. Rowe Price International Funds, Inc.</td>\n",
       "      <td>0000313212</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>811-02958</td>\n",
       "      <td>09633134</td>\n",
       "      <td>T. Rowe Price International Funds, Inc.</td>\n",
       "      <td>N-CSR (Annual shareholder report)</td>\n",
       "      <td>2009-02-25</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>Azteca Acquisition Corp</td>\n",
       "      <td>0001518749</td>\n",
       "      <td>Beverly Hills</td>\n",
       "      <td>California</td>\n",
       "      <td>333-173687</td>\n",
       "      <td>11904754</td>\n",
       "      <td>Azteca Acquisition Corp</td>\n",
       "      <td>S-1/A (Registration statement)</td>\n",
       "      <td>2011-06-10</td>\n",
       "      <td>None</td>\n",
       "      <td>htm</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000151...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5004 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    entity_name_x         cik  \\\n",
       "0     Grupo Aval Acciones Y Valores S.A.  (AVAL)   0001504764   \n",
       "1                              REPUBLIC OF CHILE   0000019957   \n",
       "2               Millburn Multi-Markets Fund L.P.   0001468910   \n",
       "3                             Ternium S.A.  (TX)   0001342874   \n",
       "4                               WisdomTree Trust   0001350487   \n",
       "...                                           ...         ...   \n",
       "4999                   Direxion Shares ETF Trust   0001424958   \n",
       "5000                               EDENOR  (EDN)   0001395213   \n",
       "5001                              DIREXION FUNDS   0001040587   \n",
       "5002     T. Rowe Price International Funds, Inc.   0000313212   \n",
       "5003                     Azteca Acquisition Corp   0001518749   \n",
       "\n",
       "                   located incorporated    file_num   film_num  \\\n",
       "0              Bogota D.c.     Colombia   001-36631  221400290   \n",
       "1           Santiago Chile        Chile   001-02574   17906270   \n",
       "2                Greenwich  Connecticut   000-54028  181178816   \n",
       "3               Luxembourg   Luxembourg   001-32734   23781454   \n",
       "4                 New York     New York  333-132380  181172160   \n",
       "...                    ...          ...         ...        ...   \n",
       "4999              New York     New York  333-150525  221112309   \n",
       "5000  City of Buenos Aires    Argentina   001-33422   23829818   \n",
       "5001              New York     New York   333-28697   18723487   \n",
       "5002             Baltimore     Maryland   811-02958   09633134   \n",
       "5003         Beverly Hills   California  333-173687   11904754   \n",
       "\n",
       "                                    entity_name_y  \\\n",
       "0     Grupo Aval Acciones Y Valores S.A.  (AVAL)    \n",
       "1                              REPUBLIC OF CHILE    \n",
       "2               Millburn Multi-Markets Fund L.P.    \n",
       "3                             Ternium S.A.  (TX)    \n",
       "4                               WisdomTree Trust    \n",
       "...                                           ...   \n",
       "4999                   Direxion Shares ETF Trust    \n",
       "5000                               EDENOR  (EDN)    \n",
       "5001                              DIREXION FUNDS    \n",
       "5002     T. Rowe Price International Funds, Inc.    \n",
       "5003                     Azteca Acquisition Corp    \n",
       "\n",
       "                                  form_file   file_date period_ending  \\\n",
       "0                      6-K (Current report)  2022-11-18    2022-11-17   \n",
       "1              18-K (Annual report) EX-99.D  2017-06-12    2016-12-31   \n",
       "2                   10-Q (Quarterly report)  2018-11-13    2018-09-30   \n",
       "3     20-F (Annual report - foreign issuer)  2023-03-30    2022-12-31   \n",
       "4            485APOS (Prospectus materials)  2018-11-09          None   \n",
       "...                                     ...         ...           ...   \n",
       "4999                       497 (Prospectus)  2022-07-28          None   \n",
       "5000  20-F (Annual report - foreign issuer)  2023-04-19    2022-12-31   \n",
       "5001         485APOS (Prospectus materials)  2018-03-29          None   \n",
       "5002      N-CSR (Annual shareholder report)  2009-02-25    2008-12-31   \n",
       "5003         S-1/A (Registration statement)  2011-06-10          None   \n",
       "\n",
       "     file_ext                                                url  \n",
       "0         htm  https://www.sec.gov/Archives/edgar/data/000150...  \n",
       "1         htm  https://www.sec.gov/Archives/edgar/data/000001...  \n",
       "2         htm  https://www.sec.gov/Archives/edgar/data/000146...  \n",
       "3         htm  https://www.sec.gov/Archives/edgar/data/000134...  \n",
       "4         htm  https://www.sec.gov/Archives/edgar/data/000135...  \n",
       "...       ...                                                ...  \n",
       "4999      htm  https://www.sec.gov/Archives/edgar/data/000142...  \n",
       "5000      htm  https://www.sec.gov/Archives/edgar/data/000139...  \n",
       "5001      htm  https://www.sec.gov/Archives/edgar/data/000104...  \n",
       "5002      htm  https://www.sec.gov/Archives/edgar/data/000031...  \n",
       "5003      htm  https://www.sec.gov/Archives/edgar/data/000151...  \n",
       "\n",
       "[5004 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8402"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chop_time_interval(start_date,end_date,frequency = \"2D\"):\n",
    "    interval = pd.interval_range(pd.Timestamp(start_date) ,pd.Timestamp(end_date),closed=\"both\",freq= frequency)\n",
    "    date_pattern = '%Y-%m-%d'\n",
    "    intervals = [((interval[i].left +timedelta(days=1)).strftime((date_pattern)), interval[i].right.strftime((date_pattern))) if i !=0 \n",
    "                 else ((interval[i].left.strftime((date_pattern)), interval[i].right.strftime((date_pattern))))\n",
    "                 for i in range(len(interval)) \n",
    "                 ]\n",
    "    return intervals\n",
    "intervals = chop_time_interval(_DATE_START,_DATE_END,'1D')\n",
    "len(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2001-01-02 00:00:00')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "interval = pd.interval_range(pd.Timestamp(_DATE_START) ,pd.Timestamp(_DATE_END),closed=\"both\",freq='2D')\n",
    "interval[0].left +timedelta(days=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
