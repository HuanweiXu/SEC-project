{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration i \n",
    "\n",
    "----\n",
    "Update python to 3.10.13 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install --yes -c defaults -c conda-forge --update-all python=3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration ii\n",
    "\n",
    "---\n",
    "\n",
    "Uninstall package fitz, which may cause import conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iii\n",
    "\n",
    "---\n",
    "\n",
    "Go to your conda's base directory(e.g. `D:\\anaconda3`),under the path `D:\\anaconda3\\Lib\\site-packages`, **delete** all folders which name contain \"fitz\" if there any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration iv\n",
    "\n",
    "---\n",
    "Use pip to install the necessary packages instead of using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp>=3.9.0 (from -r requirements.txt (line 1))\n",
      "  Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting beautifulsoup4>=4.12.2 (from -r requirements.txt (line 2))\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting Faker>=18.9.0 (from -r requirements.txt (line 3))\n",
      "  Using cached Faker-22.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pandas>=2.1.4 (from -r requirements.txt (line 4))\n",
      "  Using cached pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting regex>=2023.10.3 (from -r requirements.txt (line 5))\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tenacity>=8.2.2 (from -r requirements.txt (line 6))\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tqdm>=4.66.1 (from -r requirements.txt (line 7))\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyMuPDF>=1.23.8 (from -r requirements.txt (line 8))\n",
      "  Using cached PyMuPDF-1.23.8-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting ipykernel>=2.0.0 (from -r requirements.txt (line 9))\n",
      "  Downloading ipykernel-6.28.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting asyncio>=3.4.3 (from -r requirements.txt (line 10))\n",
      "  Using cached asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.12.2->-r requirements.txt (line 2))\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting python-dateutil>=2.4 (from Faker>=18.9.0->-r requirements.txt (line 3))\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2,>=1.22.4 (from pandas>=2.1.4->-r requirements.txt (line 4))\n",
      "  Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.1.4->-r requirements.txt (line 4))\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas>=2.1.4->-r requirements.txt (line 4))\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting PyMuPDFb==1.23.7 (from PyMuPDF>=1.23.8->-r requirements.txt (line 8))\n",
      "  Using cached PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting comm>=0.1.1 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading comm-0.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading debugpy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading ipython-8.19.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jupyter-client>=6.1.12 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading jupyter_client-8.6.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading jupyter_core-5.7.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting nest-asyncio (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading nest_asyncio-1.5.8-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting packaging (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting psutil (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading psutil-5.9.7-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting pyzmq>=24 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting tornado>=6.1 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading traitlets-5.14.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting stack-data (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting exceptiongroup (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading platformdirs-4.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.4->Faker>=18.9.0->-r requirements.txt (line 3))\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.0->aiohttp>=3.9.0->-r requirements.txt (line 1))\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel>=2.0.0->-r requirements.txt (line 9))\n",
      "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached Faker-22.0.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached PyMuPDF-1.23.8-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "Using cached PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "Downloading ipykernel-6.28.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.1/114.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading debugpy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Downloading ipython-8.19.0-py3-none-any.whl (808 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.9/808.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_core-5.7.0-py3-none-any.whl (28 kB)\n",
      "Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Downloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading traitlets-5.14.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Downloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading psutil-5.9.7-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.1/386.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, pytz, pure-eval, ptyprocess, asyncio, tzdata, traitlets, tqdm, tornado, tenacity, soupsieve, six, regex, pyzmq, PyMuPDFb, pygments, psutil, prompt-toolkit, platformdirs, pexpect, parso, packaging, numpy, nest-asyncio, multidict, idna, frozenlist, executing, exceptiongroup, decorator, debugpy, attrs, async-timeout, yarl, python-dateutil, PyMuPDF, matplotlib-inline, jupyter-core, jedi, comm, beautifulsoup4, asttokens, aiosignal, stack-data, pandas, jupyter-client, Faker, aiohttp, ipython, ipykernel\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.12\n",
      "    Uninstalling wcwidth-0.2.12:\n",
      "      Successfully uninstalled wcwidth-0.2.12\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: pure-eval\n",
      "    Found existing installation: pure-eval 0.2.2\n",
      "    Uninstalling pure-eval-0.2.2:\n",
      "      Successfully uninstalled pure-eval-0.2.2\n",
      "  Attempting uninstall: ptyprocess\n",
      "    Found existing installation: ptyprocess 0.7.0\n",
      "    Uninstalling ptyprocess-0.7.0:\n",
      "      Successfully uninstalled ptyprocess-0.7.0\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.3\n",
      "    Uninstalling tzdata-2023.3:\n",
      "      Successfully uninstalled tzdata-2023.3\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.14.0\n",
      "    Uninstalling traitlets-5.14.0:\n",
      "      Successfully uninstalled traitlets-5.14.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.4\n",
      "    Uninstalling tornado-6.4:\n",
      "      Successfully uninstalled tornado-6.4\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.5\n",
      "    Uninstalling soupsieve-2.5:\n",
      "      Successfully uninstalled soupsieve-2.5\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.10.3\n",
      "    Uninstalling regex-2023.10.3:\n",
      "      Successfully uninstalled regex-2023.10.3\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 25.1.2\n",
      "    Uninstalling pyzmq-25.1.2:\n",
      "      Successfully uninstalled pyzmq-25.1.2\n",
      "  Attempting uninstall: PyMuPDFb\n",
      "    Found existing installation: PyMuPDFb 1.23.7\n",
      "    Uninstalling PyMuPDFb-1.23.7:\n",
      "      Successfully uninstalled PyMuPDFb-1.23.7\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.17.2\n",
      "    Uninstalling Pygments-2.17.2:\n",
      "      Successfully uninstalled Pygments-2.17.2\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.6\n",
      "    Uninstalling psutil-5.9.6:\n",
      "      Successfully uninstalled psutil-5.9.6\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.41\n",
      "    Uninstalling prompt-toolkit-3.0.41:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.41\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.1.0\n",
      "    Uninstalling platformdirs-4.1.0:\n",
      "      Successfully uninstalled platformdirs-4.1.0\n",
      "  Attempting uninstall: pexpect\n",
      "    Found existing installation: pexpect 4.9.0\n",
      "    Uninstalling pexpect-4.9.0:\n",
      "      Successfully uninstalled pexpect-4.9.0\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.3\n",
      "    Uninstalling parso-0.8.3:\n",
      "      Successfully uninstalled parso-0.8.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.8\n",
      "    Uninstalling nest-asyncio-1.5.8:\n",
      "      Successfully uninstalled nest-asyncio-1.5.8\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "  Attempting uninstall: executing\n",
      "    Found existing installation: executing 2.0.1\n",
      "    Uninstalling executing-2.0.1:\n",
      "      Successfully uninstalled executing-2.0.1\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.0\n",
      "    Uninstalling exceptiongroup-1.2.0:\n",
      "      Successfully uninstalled exceptiongroup-1.2.0\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.8.0\n",
      "    Uninstalling debugpy-1.8.0:\n",
      "      Successfully uninstalled debugpy-1.8.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.2\n",
      "    Uninstalling async-timeout-4.0.2:\n",
      "      Successfully uninstalled async-timeout-4.0.2\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.3\n",
      "    Uninstalling yarl-1.9.3:\n",
      "      Successfully uninstalled yarl-1.9.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: PyMuPDF\n",
      "    Found existing installation: PyMuPDF 1.23.8\n",
      "    Uninstalling PyMuPDF-1.23.8:\n",
      "      Successfully uninstalled PyMuPDF-1.23.8\n",
      "  Attempting uninstall: matplotlib-inline\n",
      "    Found existing installation: matplotlib-inline 0.1.6\n",
      "    Uninstalling matplotlib-inline-0.1.6:\n",
      "      Successfully uninstalled matplotlib-inline-0.1.6\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter_core 5.5.0\n",
      "    Uninstalling jupyter_core-5.5.0:\n",
      "      Successfully uninstalled jupyter_core-5.5.0\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.19.1\n",
      "    Uninstalling jedi-0.19.1:\n",
      "      Successfully uninstalled jedi-0.19.1\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.2.0\n",
      "    Uninstalling comm-0.2.0:\n",
      "      Successfully uninstalled comm-0.2.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "  Attempting uninstall: asttokens\n",
      "    Found existing installation: asttokens 2.4.1\n",
      "    Uninstalling asttokens-2.4.1:\n",
      "      Successfully uninstalled asttokens-2.4.1\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.2.0\n",
      "    Uninstalling aiosignal-1.2.0:\n",
      "      Successfully uninstalled aiosignal-1.2.0\n",
      "  Attempting uninstall: stack-data\n",
      "    Found existing installation: stack-data 0.6.3\n",
      "    Uninstalling stack-data-0.6.3:\n",
      "      Successfully uninstalled stack-data-0.6.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter_client 8.6.0\n",
      "    Uninstalling jupyter_client-8.6.0:\n",
      "      Successfully uninstalled jupyter_client-8.6.0\n",
      "  Attempting uninstall: Faker\n",
      "    Found existing installation: Faker 18.9.0\n",
      "    Uninstalling Faker-18.9.0:\n",
      "      Successfully uninstalled Faker-18.9.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.0\n",
      "    Uninstalling aiohttp-3.9.0:\n",
      "      Successfully uninstalled aiohttp-3.9.0\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.18.1\n",
      "    Uninstalling ipython-8.18.1:\n",
      "      Successfully uninstalled ipython-8.18.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.27.1\n",
      "    Uninstalling ipykernel-6.27.1:\n",
      "      Successfully uninstalled ipykernel-6.27.1\n",
      "Successfully installed Faker-22.0.0 PyMuPDF-1.23.8 PyMuPDFb-1.23.7 aiohttp-3.9.1 aiosignal-1.3.1 asttokens-2.4.1 async-timeout-4.0.3 asyncio-3.4.3 attrs-23.2.0 beautifulsoup4-4.12.2 comm-0.2.1 debugpy-1.8.0 decorator-5.1.1 exceptiongroup-1.2.0 executing-2.0.1 frozenlist-1.4.1 idna-3.6 ipykernel-6.28.0 ipython-8.19.0 jedi-0.19.1 jupyter-client-8.6.0 jupyter-core-5.7.0 matplotlib-inline-0.1.6 multidict-6.0.4 nest-asyncio-1.5.8 numpy-1.26.3 packaging-23.2 pandas-2.1.4 parso-0.8.3 pexpect-4.9.0 platformdirs-4.1.0 prompt-toolkit-3.0.43 psutil-5.9.7 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.17.2 python-dateutil-2.8.2 pytz-2023.3.post1 pyzmq-25.1.2 regex-2023.12.25 six-1.16.0 soupsieve-2.5 stack-data-0.6.3 tenacity-8.2.3 tornado-6.4 tqdm-4.66.1 traitlets-5.14.1 tzdata-2023.4 wcwidth-0.2.13 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -r requirements.txt --upgrade --force-reinstal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymupdf --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration v\n",
    "\n",
    "---\n",
    "press **Ctrl** + **SHFIT** + **P**, type **reload window** and select to reboot the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration vi\n",
    "\n",
    "---\n",
    "Following Section only need to be run at the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import ClientSession\n",
    "import json, regex\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "async def __constant_update():\n",
    "    async with ClientSession(raise_for_status=True,headers=headers) as c:\n",
    "        async with c.get(\"https://www.sec.gov/edgar/search/js/edgar_full_text_search.js\") as res:\n",
    "            _script = await res.text()\n",
    "\n",
    "        with open(\"constants.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"_FORMS = \")\n",
    "            json.dump({\n",
    "                form.pop(\"form\"): form\n",
    "                for form in eval(regex.search(\n",
    "                    R\"^const forms = (\\[\\r?\\n(?: {4}\\{.*?\\},*\\r?\\n)*(?: {4}\\{.*?\\})\\r?\\n\\])\\.sort\",\n",
    "                    _script,\n",
    "                    regex.MULTILINE\n",
    "                )[1])\n",
    "            }, f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "            f.write(\"_LOCATIONS = \")\n",
    "            json.dump(dict(eval(regex.search(\n",
    "                R\"^const locationsArray = (\\[\\r?\\n(?: {4}\\[.*?\\],\\r?\\n)*(?: {4}\\[.*?\\])\\r?\\n\\]);\",\n",
    "                _script,\n",
    "                regex.MULTILINE\n",
    "            )[1])), f, indent=4)\n",
    "\n",
    "            f.write('\\n')\n",
    "await __constant_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import date , timedelta\n",
    "from itertools import zip_longest\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from aiohttp import ClientSession,ClientResponseError\n",
    "from faker import Faker\n",
    "import regex\n",
    "from tenacity import retry,stop_after_attempt, wait_fixed,after_log,RetryError\n",
    "#import retry\n",
    "import time\n",
    "from tqdm import *\n",
    "from constants import _FORMS, _LOCATIONS\n",
    "import os\n",
    "import io\n",
    "import fitz\n",
    "_DISPLAY_NAME_REGEX = regex.compile(R\"(.*) \\(CIK (\\d{10})\\)\", regex.V1)\n",
    "_CC_REGEX = regex.compile(R\"[\\p{Cc}\\p{Cf}]+\", regex.V1)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_RETRY_SC = {403, 500, 502, 503, 504}\n",
    "_DEFAULT_BYTE = b'{\"took\":530,\"timed_out\":false,\"_shards\":{\"total\":50,\"successful\":50,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":0,\"relation\":\"eq\"},\"max_score\":null,\"hits\":[]},\"aggregations\":{\"entity_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"sic_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"biz_states_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]},\"form_filter\":{\"doc_count_error_upper_bound\":0,\"sum_other_doc_count\":0,\"buckets\":[]}},\"query\":\"{\\\\\"query\\\\\":{\\\\\"bool\\\\\":{\\\\\"must\\\\\":[{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"data\\\\\"}},{\\\\\"match_phrase\\\\\":{\\\\\"doc_text\\\\\":\\\\\"breach\\\\\"}}],\\\\\"must_not\\\\\":[],\\\\\"should\\\\\":[],\\\\\"filter\\\\\":[{\\\\\"terms\\\\\":{\\\\\"ciks\\\\\":[\\\\\"0001199046\\\\\"]}},{\\\\\"range\\\\\":{\\\\\"file_date\\\\\":{\\\\\"gte\\\\\":\\\\\"2024-01-02\\\\\",\\\\\"lte\\\\\":\\\\\"2024-01-02\\\\\"}}}]}},\\\\\"from\\\\\":0,\\\\\"size\\\\\":100,\\\\\"aggregations\\\\\":{\\\\\"form_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"root_form\\\\\",\\\\\"size\\\\\":30}},\\\\\"entity_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"display_names.raw\\\\\",\\\\\"size\\\\\":30}},\\\\\"sic_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"sics\\\\\",\\\\\"size\\\\\":30}},\\\\\"biz_states_filter\\\\\":{\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"biz_states\\\\\",\\\\\"size\\\\\":30}}}}\"}'\n",
    "Boolean_KeyWord = lambda para: \" \".join(f\"\\\"{phrase}\\\"\" for phrase in para) if isinstance(para,list) else f\"\\\"{para}\\\"\"\n",
    "_TRAFFIC_LOCK = None\n",
    "_TEST = []\n",
    "@retry(wait=wait_fixed(1))\n",
    "async def fetch(fetch_bar,semaphore,client,phrases,cik,end,forms,start='2001-01-01',range = 'custom',category= 'custom',entity=None,): #'https://efts.sec.gov/LATEST/search-index? \n",
    "    data = {'q':phrases,\n",
    "            'startdt':start,\n",
    "            'enddt':end,\n",
    "            'ciks':cik,\n",
    "            'dataRange':'custom',\n",
    "            'category':'custom',\n",
    "            'forms':forms}\n",
    "    url = 'https://efts.sec.gov/LATEST/search-index'\n",
    "    async with semaphore,client.request(method='get',url=url,params = data) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 200:\n",
    "            result = await res.read()\n",
    "            fetch_bar.update(1)\n",
    "            return result#await res.json()\n",
    "        raise ValueError(f\"Status Code = {res.status}\")\n",
    "    \n",
    "def _concat_to_url(cik: str, adsh: str, filename: str) -> str:\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{adsh}/{filename}\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(6), wait=wait_fixed(2),after=after_log(_LOGGER, logging.WARNING),reraise=False)#, \n",
    "async def _download(semaphore: asyncio.Semaphore, row_index, df,client,keywords,download_bar):\n",
    "    url = df.loc[row_index,\"url\"]\n",
    "    ext = df.loc[row_index,\"file_ext\"]\n",
    "    if ext.lower() not in [\"htm\",\"pdf\",\"txt\"]:\n",
    "        _LOGGER.warning(f\"Unkown extension in row [{row_index}]\")\n",
    "        tem = f\"paragrah{1}\"\n",
    "        df.loc[row_index,tem] = 'Unknow extension'\n",
    "        return download_bar.update(1)\n",
    "      \n",
    "    async with semaphore, client.get(url) as res:\n",
    "        await asyncio.sleep(1)\n",
    "        if res.status == 404:\n",
    "            _LOGGER.warning(f\"URL: {url} is not exist - ({res.status}) {res.reason}. Skipping download.\")\n",
    "            return download_bar.update(1)\n",
    "        if res.status == 403 or res.status == 429:\n",
    "            if not _TRAFFIC_LOCK :\n",
    "                _LOGGER.warning(f\"Reach request limitation: {url}: ({res.status}) {res.reason}.\\n Restart to crawl after 10 minutes. \")\n",
    "                _TRAFFIC_LOCK = time.time()\n",
    "                time.sleep(620)\n",
    "            while True:\n",
    "                confirm = input(\"Please open 'https://www.sec.gov/edgar/search/' in your browser, press 'ok' to crawl continue if it available\")\n",
    "                if confirm.lower() == \"ok\":\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Manully confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        _LOGGER.warning(f\"Fail to Manully confirm: ({res.status}) {res.reason} \")\n",
    "                        _TRAFFIC_LOCK = time.time()\n",
    "                        \n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - _TRAFFIC_LOCK \n",
    "                if elapsed_time >= 180 :\n",
    "                    if res.ok:\n",
    "                        _LOGGER.warning(\"Automatically confirm. Continue to crawl \")\n",
    "                        break\n",
    "                    else:\n",
    "                        _LOGGER.warning(\"Fail to automate confirm. Will retry again after 10 mintues\")\n",
    "                        time.sleep(620)\n",
    "                        _TRAFFIC_LOCK  = time.time()\n",
    "                        \n",
    "        if res.ok:\n",
    "            try:\n",
    "                html = await res.read()\n",
    "                if ext == \"htm\":\n",
    "                    paragraphs = extract_html(html,keywords)\n",
    "                elif ext == \"pdf\":\n",
    "                    paragraphs = extract_pdf(html,keywords)\n",
    "                elif ext == \"txt\":\n",
    "                    paragraphs = extract_txt(html,keywords)\n",
    "                #paragraphs = extract_contents(bytes,keywords,ext)\n",
    "                if not paragraphs:\n",
    "                    _LOGGER.warning(f\"No content extracted from {url}\")\n",
    "                    tem = f\"paragrah{1}\"\n",
    "                    df.loc[row_index, tem] = 'No content extracted'\n",
    "                    return download_bar.update(1)\n",
    "    \n",
    "                for num in range(len(paragraphs)):\n",
    "                    tem = f\"paragrah{num + 1}\"\n",
    "                    df.loc[row_index,tem] = str(paragraphs[num])\n",
    "                return download_bar.update(1)\n",
    "            except RetryError:\n",
    "                _LOGGER.warning(f\"Retring download url: {url} at the row: {row_index}\")\n",
    "                return download_bar.update(1)\n",
    "\n",
    "    _LOGGER.warning(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    tem = f\"paragrah{1}\"\n",
    "    df.loc[row_index, tem] = 'Download skipped'\n",
    "    #raise Exception(f\"Failed to download {url}: ({res.status}) {res.reason}\")\n",
    "    return download_bar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "def _parse_display_name(s: str, cik: str):\n",
    "    if s is not None and (m := _DISPLAY_NAME_REGEX.fullmatch(s)):\n",
    "        if (scik := m[2]) != cik:\n",
    "            _LOGGER.warning(f\"mismatched CIK: {scik} (parsed from \\\"{s}\\\") v.s. {cik}\")\n",
    "        return m[1], scik\n",
    "    return s, cik\n",
    "\n",
    "def _parse_hit(hit: Dict[str, Any]): \n",
    "    _id = hit[\"_id\"]\n",
    "    source = hit[\"_source\"]\n",
    "    adsh, filename = _id.split(':')\n",
    "    filename_main, filename_ext = filename.rsplit('.', 1)\n",
    "    xsl = source[\"xsl\"]\n",
    "    \n",
    "    if xsl and filename_ext.lower() == \"xml\":\n",
    "        filename_main = f\"{xsl}/{filename_main}\"\n",
    "    filename = f\"{filename_main}.{filename_ext}\"\n",
    "\n",
    "    file_nums = source[\"file_num\"]\n",
    "    film_nums = source[\"film_num\"]\n",
    "    rows = pd.DataFrame((\n",
    "        [_id, *_parse_display_name(display_name, cik), str(loc).split(\",\")[0], _LOCATIONS.get(code), file_num, film_num]\n",
    "        for display_name, cik, loc, code, file_num, film_num in zip_longest(\n",
    "            source[\"display_names\"],\n",
    "            source[\"ciks\"],\n",
    "            source[\"biz_locations\"],\n",
    "            source[\"biz_states\"], #source[\"inc_states\"] if source[\"inc_states\"] else \n",
    "            file_nums if isinstance(file_nums, list) else [file_nums] if file_nums else (),\n",
    "            film_nums if isinstance(film_nums, list) else [film_nums] if film_nums else ()\n",
    "        ) \n",
    "    ), columns=[\"id\", \"entity_name\", \"cik\", \"located\", \"incorporated\", \"file_num\", \"film_num\"], copy=False)#, dtype=str\n",
    "    form = source[\"form\"]\n",
    "    root_form = source[\"root_form\"]\n",
    "    form_title = \"\"\n",
    "    if root_form in _FORMS:\n",
    "        form_title = f\" ({_FORMS[root_form]['title']})\"\n",
    "    file_type = source[\"file_type\"]\n",
    "    if not file_type:\n",
    "        file_type = source[\"file_description\"]\n",
    "    if not file_type:\n",
    "        file_type = filename\n",
    "    ciks = rows.loc[0,\"cik\"]\n",
    "    info = pd.DataFrame({\n",
    "        \"entity_name\":rows['entity_name'],\n",
    "        \"id\": _id,\n",
    "        \"form_file\": f\"{form}{form_title}{'' if form == file_type else f' {file_type}'}\",\n",
    "        \"file_date\": source[\"file_date\"],\n",
    "        \"period_ending\": source.get(\"period_ending\", None),\n",
    "        \"file_ext\": filename_ext,\n",
    "        \"url\": _concat_to_url(ciks, adsh.replace('-', ''), filename),\n",
    "        \"parser\": None#getattr(parsers, f\"_parse_{filename_ext.lower()}\", None)\n",
    "    },copy=False,dtype=str)#, dtype=object\n",
    "    \n",
    "    result = pd.merge(rows,info,how=\"left\",on=\"id\")\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "#len_check = lambda chunk,sub_delimiter: [chunk] if len(chunk) <=3000 else chunk.split(sub_delimiter)\n",
    "def extract_contents(fetch,keywords,ext):\n",
    "    start = time.time()\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=ext)\n",
    "\n",
    "    total = [para.replace(\"\\n\",\" \") for page in range(pdf_document.page_count) \n",
    "            for lists in pdf_document[page].get_text(\"blocks\") \n",
    "            for  para in lists \n",
    "            if any(keyword in str(para).lower().replace(\"-\",\" \") for keyword in keywords) ]\n",
    "    _TEST.append(time.time() - start)\n",
    "    return total\n",
    "\n",
    "def extract_html(html, keywords):\n",
    "    matching_paragraphs = [regex.sub(r\"(\\s+)\",\" \",para).strip() \n",
    "                           for para in BeautifulSoup(html, \"lxml\",from_encoding='utf-8').get_text('\\n\\n').split('\\n\\n') \n",
    "                           if any(keyword in para.lower().replace(\"-\",\" \").replace(\"\\n\",\" \") \n",
    "                            for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "def extract_pdf(fetch,keywords):\n",
    "    start = time.time()\n",
    "    pdf_stream = io.BytesIO(fetch)\n",
    "    pdf_document = fitz.open(stream=pdf_stream,filetype=\"pdf\")\n",
    "\n",
    "    total = [para.replace(\"\\n\",\" \") for page in range(pdf_document.page_count) \n",
    "            for lists in pdf_document[page].get_text(\"blocks\") \n",
    "            for  para in lists \n",
    "            if any(keyword in str(para).lower().replace(\"-\",\" \") for keyword in keywords) ]\n",
    "    _TEST.append(time.time() - start)\n",
    "    return total\n",
    "\n",
    "def extract_txt(bytes,keywords):\n",
    "    txt = bytes.decode('utf-8').split('\\n\\n')\n",
    "    txt = [i.replace(\"\\n\",\" \")for i in txt]\n",
    "    matching_paragraphs = [paragraph for paragraph in txt if any(keyword in paragraph.lower().replace(\"-\",\" \") for keyword in keywords)]\n",
    "    return matching_paragraphs\n",
    "\n",
    "    \n",
    "def chop_time_interval(start_date,end_date,frequency = \"2D\"):\n",
    "    interval = pd.interval_range(pd.Timestamp(start_date) ,pd.Timestamp(end_date),closed=\"both\",freq= frequency)\n",
    "    date_pattern = '%Y-%m-%d'\n",
    "    intervals = [((interval[i].left + timedelta(days=1)).strftime((date_pattern)), interval[i].right.strftime((date_pattern))) if i !=0 \n",
    "                 else ((interval[i].left.strftime((date_pattern)), interval[i].right.strftime((date_pattern))))\n",
    "                 for i in range(len(interval)) \n",
    "                 ]\n",
    "    return intervals\n",
    "\n",
    "def decode(byte):\n",
    "    total_hits = json.loads(byte.decode('utf-8'))['hits']['total']['value']\n",
    "    if total_hits >= 100:\n",
    "        _LOGGER.warning(f\"Numbers of Hits/entries is : '{total_hits}' >= 100. Try to reduce the 'interval of data range' or 'numerbes of per query' to avoid data missing.\")\n",
    "    hits =  json.loads(byte.decode('utf-8'))[\"hits\"][\"hits\"]\n",
    "    return hits\n",
    "def chop_ciks(\n",
    "    ciks: Optional[Union[Path, int, str, List[Any]]],\n",
    "    ciks_per_query: int\n",
    ") -> Generator[Optional[List[str]], None, None]:\n",
    "    # defaults to None\n",
    "    _ciks: Optional[List[str]] = None\n",
    "    # if the provided parameter is a Path, read the CIKs from the file\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "\n",
    "    if _ciks:\n",
    "        for i in range(0, len(_ciks), ciks_per_query):\n",
    "            yield _ciks[i:i + ciks_per_query]\n",
    "    else:\n",
    "        yield None\n",
    "        \n",
    "def CIK(ciks):\n",
    "    if isinstance(ciks, Path):\n",
    "        try:\n",
    "            with open(ciks, \"r\", encoding=\"UTF-8\") as f:\n",
    "                try:\n",
    "                    _ciks = [f\"{int(cik):010}\" for cik in f.read().splitlines()]\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"{ciks} contains invalid CIKs\") from e\n",
    "        except IOError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid file\") from e\n",
    "    # if it's an iterable of values, treat all values as CIKs\n",
    "    elif isinstance(ciks, list):\n",
    "        try:\n",
    "            _ciks = [f\"{int(cik):010}\" for cik in ciks]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK list\") from e\n",
    "    # if it's a single string, consider it as a single CIK\n",
    "    elif isinstance(ciks, str):\n",
    "        try:\n",
    "            _ciks = [f\"{int(ciks):010}\"]\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"{ciks} is not a valid CIK\") from e\n",
    "    # same as previous with the preferred (int) type\n",
    "    elif isinstance(ciks, int):\n",
    "        _ciks = [f\"{ciks:010}\"]\n",
    "    if _ciks == []:\n",
    "        _ciks = \"\"\n",
    "    return _ciks\n",
    "    \n",
    "async def main(_PHRASES,\n",
    "               _FILING_TYPES,\n",
    "               _DATE_START,\n",
    "               _DATE_END,\n",
    "               _CIKS_PER_QUERY, \n",
    "               _CIKS,\n",
    "               _buffer_chunk_size,\n",
    "               df,headers,\n",
    "               _OUTPUT_NAME,\n",
    "               _OUTPUT_FORMAT,\n",
    "               _DATA_RANGE_INTERVAL=\"2D\"):\n",
    "    \n",
    "    phrases = [Boolean_KeyWord(para) for para in _PHRASES ]\n",
    "    \n",
    "    if _OUTPUT_FORMAT not in [\"excel\",\"xlsx\",\"csv\"]:\n",
    "        raise TypeError(f\"{_OUTPUT_FORMAT} is not a valid extension. Options extension [excel,xlsx,csv].\")\n",
    "    semaphore = asyncio.Semaphore(20)\n",
    "    if _FILING_TYPES == []:\n",
    "        _FILING_TYPES = [\"\"]\n",
    "    #-------------- Crawl --------\n",
    "\n",
    "    async with ClientSession(raise_for_status=False, headers=headers) as client :\n",
    "        #------Fetch--------\n",
    "        if CIK(_CIKS):\n",
    "            import math\n",
    "            total = math.ceil(len(CIK(_CIKS)) / _CIKS_PER_QUERY * len(phrases))\n",
    "            with tqdm(        \n",
    "                total=total) as fetch_bar:\n",
    "                print(\"Starting fetch...\")\n",
    "                fetch_tasks = [\n",
    "                        asyncio.create_task(fetch(\n",
    "                            semaphore=semaphore,\n",
    "                            client=client,\n",
    "                            phrases=phrase,\n",
    "                            cik=ciks,\n",
    "                            start=_DATE_START,\n",
    "                            end=_DATE_END,\n",
    "                            forms=_FILING_TYPES,\n",
    "                            fetch_bar=fetch_bar\n",
    "                        ))\n",
    "                        for ciks in chop_ciks(_CIKS,_CIKS_PER_QUERY)\n",
    "                        for phrase in phrases\n",
    "                    ]\n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        else:\n",
    "            intervals = chop_time_interval(_DATE_START,_DATE_END,_DATA_RANGE_INTERVAL)\n",
    "            with tqdm(total=len(intervals) * len(phrases)) as fetch_bar:\n",
    "                fetch_tasks = [asyncio.create_task(fetch(\n",
    "                                    semaphore=semaphore,\n",
    "                                    client=client,\n",
    "                                    phrases=phrase,\n",
    "                                    cik=\"\",\n",
    "                                    start=start,\n",
    "                                    end=end,\n",
    "                                    forms=_FILING_TYPES,\n",
    "                                    fetch_bar=fetch_bar\n",
    "                                )) \n",
    "                               for start,end in intervals\n",
    "                               for phrase in phrases]\n",
    "                \n",
    "                fetched_data = await asyncio.gather(*fetch_tasks)\n",
    "        df = pd.concat([df] + \n",
    "                    [_parse_hit(hit) for data in fetched_data \n",
    "                     for hit in decode(data)],\n",
    "                    ignore_index=True)\n",
    "        \n",
    "        if df.shape[0] == 0:\n",
    "            _LOGGER.warning(\"Fetch completed with 0 result. Program existing.\")\n",
    "            return\n",
    "\n",
    "        \n",
    "        df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "        df = df.sample(frac=1.0)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        if _FILING_TYPES: # Filter filing types\n",
    "            df = df[df['form_file'].apply(lambda x: any(keyword in x for keyword in _FILING_TYPES))]\n",
    "            if df.shape[0] == 0:\n",
    "                _LOGGER.warning(f'Invalid FILING_TYPES: {_FILING_TYPES}, program exsiting. ')\n",
    "        #-----Docs download-------\n",
    "        print(f\"fetch completed and collected [{df.shape[0]}] of docs,starting download docs..\")\n",
    "        #keywords = [keyword for lists in _PHRASES for keyword in lists]\n",
    "        total = df.shape[0]\n",
    "        keywords = [keyword for lists in _PHRASES for keyword in lists]\n",
    "        #semaphore = asyncio.Semaphore(40)\n",
    "        with tqdm(total=total) as download_bar:\n",
    "            for index in range(0,total,_buffer_chunk_size):\n",
    "                index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "                download_tasks = [\n",
    "                    asyncio.create_task(_download(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        df = df,\n",
    "                        row_index=row,\n",
    "                        download_bar=download_bar,\n",
    "                        keywords= keywords)\n",
    "                    )for row in index_range\n",
    "                    ]\n",
    "\n",
    "                downloaded = await asyncio.gather(*download_tasks)     \n",
    "\n",
    "    #df.dropna(how=\"all\", inplace=True)\n",
    "    del df[\"parser\"] \n",
    "    del df['id']\n",
    "    if _OUTPUT_FORMAT in [\"excel\",\"xlsx\"]:\n",
    "        df.to_excel(f\"{_OUTPUT_NAME}.xlsx\")\n",
    "    elif _OUTPUT_FORMAT.lower() == \"csv\":      \n",
    "        df.to_csv(f\"{_OUTPUT_NAME}.csv\")\n",
    "    print(f\"Data have been export at {os.getcwd()}\") \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters\n",
    "\n",
    "This section defines all customisable parameters.\n",
    "- **df** (`pandas dataframe`) : Output data frame.\n",
    "\n",
    "- **headers** (`str`) : A string of user's email address for declaration.\n",
    "\n",
    "- **PHRASES** (`List[list,str]`): A list of lists that  keywords or phrases to search for. Every word in a nested list represents the 'and' boolean (appear at the same time) of the nested list.\n",
    "\n",
    "    ---\n",
    "    E.g. \n",
    "    1. [ [\"data breach\", \"cyber security\"], [\"fiscal deficit\"] ] = (\"data breach\" **and** \"cyber security\") **or** \"fiscal deficit\"\n",
    "    \n",
    "    2. [ [\"data breach\"], [\"cyber security\"], [\"fiscal deficit\"] ] = \"data breach\" **or** \"cyber security\" **or** \"fiscal deficit\" \n",
    "    \n",
    "     **as the same as** [\"data breach\", \"cyber security\",\"fiscal deficit\"] = \"data breach\" **or** \"cyber security\" **or** \"fiscal deficit\", ***while this input format may casue unexpected result and not recommanded***.\n",
    "    \n",
    "    ---\n",
    "\n",
    "\n",
    "\n",
    "- **DATE_START** & **DATE_END** (both `date`): As indicated by the name. But it should conform to the ISO time format, i.e., YYYY-MM-DD as shown in the example.\n",
    "\n",
    "- **DATA_RANGE_INTERVAL** (`str`):\n",
    "    -  A string to set up a data range interval for per query. Enable when do not providing an specific CIKs input, i.e., \"1D\",\"3D\" and \"**X**D\".\n",
    "    -  Reducing the interval will result in more queires have to be made. Recommanded set as not more than \"5D\" since may return more than 100 enties per query and casue data missing.\n",
    "\n",
    "- **FILING_TYPES** (`List[str]`): A list of filling types.\n",
    "\n",
    "- **CIKS** (`Optional[Union[Path, int, str, List[Union[int, str]]]]`): \n",
    "    - A list of CIKs in no more than 10 digits, or it can be a path to the file containing all CIKs for the query. \n",
    "    - Can be set either specify(e.g, path/list/integer) or empty. \n",
    "    - If it set as empty, query will base on ***date*** instead of CIKs.\n",
    "\n",
    "- **CIKS_PER_QUERY** (`int`): \n",
    "    - Controls the number of CIKs included in one query. \n",
    "    - Recommended value is 1. By the most recent testing(4 Jan 2024), I found out that server will only return information of **the last CIK** if we query multiple CIKs each time.\n",
    "\n",
    "- **BUFFER_CHUNK_SIZE** (`int`): The maximum number of files allowed to be cached in the memory.\n",
    "\n",
    "- **OUTPUT_NAME**: The file name without the suffix of the output file.\n",
    "\n",
    "- **OUTPUT_FORMAT** (`enumerate[\"excel\",\"xlsx\",\"csv\"]`): The file format of the output file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:35<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch completed and collected [1098] of docs,starting download docs..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 54/1098 [00:41<15:25,  1.13it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "T0 = time.time()\n",
    "df = pd.DataFrame()\n",
    "headers = {\"User-Agent\":\"a1835057@student.adelaide.edu.au\"}\n",
    "_PHRASES = [[\"data breach\", \"cyber security\"],[\"fiscal deficit\"]]\n",
    "#_PHRASES = [[\"data breach\", \"cyber security\"]]\n",
    "_DATE_START = \"2023-06-01\"\n",
    "_DATE_END = \"2024-01-08\"\n",
    "_FILING_TYPES = []\n",
    "### Query based on date\n",
    "_DATA_RANGE_INTERVAL = \"3D\"\n",
    "_CIKS =  [] \n",
    "### Query Base on CIks\n",
    "#_CIKS = Path(\"sample_input_file.txt\") ####Please mannually select N here for Testing for the first N CIKs###\n",
    "_CIKS_PER_QUERY = 1\n",
    "####################\n",
    "_BUFFER_CHUNK_SIZE = 1000\n",
    "_OUTPUT_NAME = f\"{_DATE_START}_to_{_DATE_END }_withCIKs_{_CIKS_PER_QUERY}PerQuery_Boolean\"\n",
    "# _OUTPUT_FORMAT = \"csv\"\n",
    "_OUTPUT_FORMAT = \"excel\"\n",
    "\n",
    "\n",
    "df = await main(\n",
    "    _PHRASES,\n",
    "    _FILING_TYPES,\n",
    "    _DATE_START,\n",
    "    _DATE_END,\n",
    "    _CIKS_PER_QUERY,\n",
    "    _CIKS,\n",
    "    _BUFFER_CHUNK_SIZE,\n",
    "    df,\n",
    "    headers,\n",
    "    _OUTPUT_NAME,\n",
    "    _OUTPUT_FORMAT,\n",
    "    _DATA_RANGE_INTERVAL\n",
    ")\n",
    " \n",
    "END = time.time()\n",
    "print(\"--\"*20,\n",
    "      f\"All tasks completed! Time Cost:{round((END-T0)/60,1)} minutes \",sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:55<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_excel(\"20240102_noCKIs_full.xlsx\")\n",
    "df = df[df[\"file_ext\"] == \"pdf\"].reset_index(drop=True)\n",
    "#df.drop_duplicates(subset=\"id\",inplace=True)\n",
    "#df = df.sample(frac=1.0)\n",
    "total = df.shape[0]\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "_buffer_chunk_size = 100\n",
    "_TEST = []\n",
    "#headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\"}\n",
    "headers = {\"User-Agent\":\"a1835057@student.adelaide.edu.au\"}\n",
    "_PHRASES = [[\"data breach\",\"cyber security\"]]\n",
    "async with ClientSession(raise_for_status=True, headers=headers) as client :\n",
    "        with tqdm(total=total) as download_bar:\n",
    "            for index in range(0,total,_buffer_chunk_size):\n",
    "                index_range = list(range(index,min(index+_buffer_chunk_size,df.shape[0])))\n",
    "                download_tasks = [\n",
    "                    asyncio.create_task(_download(\n",
    "                        semaphore=semaphore,\n",
    "                        client=client,\n",
    "                        df = df,\n",
    "                        row_index=row,\n",
    "                        download_bar=download_bar,\n",
    "                        keywords=[\"data breach\",\"cyber security\"])\n",
    "                    )for row in index_range\n",
    "                    ]\n",
    "\n",
    "                downloaded = await asyncio.gather(*download_tasks)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29707265817202055"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31359158079447214"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
